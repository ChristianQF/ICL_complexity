# -*- coding: utf-8 -*-
"""2S_Network_analysis-F.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13MbU1QixLod85ynXwl-KjzNRnqpi88bG
"""

# Cargar el Drive helper y mount
from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Formacion/Master_Complex_Systems/Complex_Networks/Project

"""## Install Libraries"""

!pip install powerlaw

!pip install py3Dmol

!pip install Infomap

!pip install Bio

!pip install biopython propka

"""## Configuration"""

import json
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
import matplotlib
import numpy as np
from collections import Counter
import seaborn as sns

# ConfiguraciÃ³n de estilo para las visualizaciones
plt.style.use('seaborn-v0_8-darkgrid')
matplotlib.rcParams['figure.figsize'] = [12, 8]
matplotlib.rcParams['font.size'] = 12

"""## Functions

### I. Generation of networks
"""

# ============================================
# 1. UPLOAD AND PROCESS JSON ARCHIVES
# ============================================

def load_ring_json_data(json_path):
    """
    Loads and processes the JSON file in GraphJSON format
    """
    with open(json_path, 'r') as f:
        data = json.load(f)

    print(f"File loaded successfully")
    print(f"Main keys: {list(data.keys())}")

    # Basic information
    print(f"Directed graph: {data.get('directed', False)}")
    print(f"Multigraph: {data.get('multigraph', False)}")

    return data

# ============================================
# 2. EXTRACT NODES AND EDGES
# ============================================

def extract_nodes_edges(data):
    """
    Extracts nodes and edges from GraphJSON format
    """
    # Extract nodes
    nodes = []
    if 'elements' in data and 'nodes' in data['elements']:
        for node in data['elements']['nodes']:
            node_info = node['data']
            nodes.append({
                'id': node_info['id'],
                'name': node_info.get('name', ''),
                'residue': f"{node_info['chain']}/{node_info['resi']}/{node_info['resn']}",
                'chain': node_info['chain'],
                'resi': node_info['resi'],
                'resn': node_info['resn'],
                'degree': node_info.get('degree', 0),
                'dssp': node_info.get('dssp', ''),
                'x_coord': node_info.get('x_coord', 0),
                'y_coord': node_info.get('y_coord', 0),
                'z_coord': node_info.get('z_coord', 0)
            })

    print(f"Number of nodes extracted: {len(nodes)}")

    # Extract edges (interactions)
    edges = []
    if 'elements' in data and 'edges' in data['elements']:
        for edge in data['elements']['edges']:
            edge_info = edge['data']
            edges.append({
                'source': edge_info['source'],
                'target': edge_info['target'],
                'interaction': edge_info.get('interaction', ''),
                'type': edge_info.get('type', ''),
                'distance': edge_info.get('distance', 0),
                'probability': edge_info.get('probability', 1.0),
                'energy': edge_info.get('energy', 0)
            })
    elif 'data' in data and len(data['data']) > 0:
        # Alternative format: edges in 'data'
        for edge in data['data']:
            edges.append({
                'source': edge['source'],
                'target': edge['target'],
                'interaction': edge.get('interaction', ''),
                'type': edge.get('type', ''),
                'distance': edge.get('distance', 0),
                'probability': edge.get('probability', 1.0),
                'energy': edge.get('energy', 0)
            })

    print(f"Number of edges (interactions) extracted: {len(edges)}")

    return nodes, edges

# ============================================
# 3. CREAR GRAFO CON NETWORKX
# ============================================

def create_graph_from_dataframes(df_nodos, df_aristas):
    """
    Crea un grafo de NetworkX a partir de DataFrames de nodos y aristas
    """
    G = nx.Graph()

    # Agregar nodos con atributos
    for _, nodo in df_nodos.iterrows():
        G.add_node(
            nodo['id'],
            residue=nodo['residue'],
            chain=nodo['chain'],
            resi=nodo['resi'],
            resn=nodo['resn'],
            degree=nodo['degree'],
            dssp=nodo['dssp'],
            x_coord=nodo['x_coord'],
            y_coord=nodo['y_coord'],
            z_coord=nodo['z_coord']
        )

    # Agregar aristas con atributos
    for idx, arista in df_aristas.iterrows():
        G.add_edge(
            arista['source'],
            arista['target'],
            interaction=arista['interaction'],
            type=arista['type'],
            distance=arista['distance'],
            probability=arista['probability'],
            energy=arista.get('energy', 0),
            id=idx
        )

    return G

"""### II. Analisis of Networks

**clustering coefficient analysis**
"""

def analyze_clustering(G):
    """
    Analyzes the clustering coefficient of a graph.
    Args: G: NetworkX graph
    Returns: DataFrame with clustering per node and displays plots
    """
    # Calculate clustering per node
    node_clustering = nx.clustering(G)

    # Convert to DataFrame
    df_clustering = pd.DataFrame(
        list(node_clustering.items()),
        columns=['Node', 'Clustering_Coefficient']
    )

    # Calculate average clustering
    average_clustering = np.mean(list(node_clustering.values()))

    # Create figure with two subplots
    plt.subplots(1, 1, figsize=(6, 4))

    # Histogram
    plt.hist(list(node_clustering.values()), bins=20,
                 edgecolor='black', alpha=0.7, color='skyblue')
    plt.xlabel('Clustering Coefficient')
    plt.ylabel('Frequency')
    plt.title('Clustering Coefficient Distribution')
    plt.axvline(average_clustering, color='red', linestyle='--',
                   label=f'Average: {average_clustering:.3f}')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    # Print results
    print(f"Average Clustering Coefficient: {average_clustering:.4f}")
    print(f"Number of nodes: {G.number_of_nodes()}")
    print(f"Number of edges: {G.number_of_edges()}")

    return df_clustering

from collections import Counter

def analyze_distances(G):
    """
    Analyzes distances in a graph: average path length and diameter.
    Args: G: NetworkX graph (should be connected or the giant component will be analyzed)
    Returns: Dictionary with distance metrics
    """
    # If graph is not connected, use the giant component
    if not nx.is_connected(G):
        print("Graph not connected. Using giant component...")
        G = G.subgraph(max(nx.connected_components(G), key=len)).copy()
        print(f"Nodes in giant component: {G.number_of_nodes()}")

    # Calculate all shortest distances
    print("Calculating distances... (may take time for large graphs)")
    paths = dict(nx.all_pairs_shortest_path_length(G))

    # Extract all distances (excluding distance 0 to itself)
    all_distances = []
    for source in paths:
        for target in paths[source]:
            if source != target:
                all_distances.append(paths[source][target])

    # Calculate metrics
    avg_path_length = np.mean(all_distances)
    max_distance = np.max(all_distances)  # Actual diameter
    distances_counter = Counter(all_distances)

    # Effective diameter (90th percentile)
    sorted_dists = sorted(all_distances)
    idx_90 = int(0.9 * len(sorted_dists))
    effective_diameter = sorted_dists[idx_90]

    # Create plots
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))

    # Distance histogram
    dist_values = sorted(distances_counter.keys())
    dist_counts = [distances_counter[d] for d in dist_values]

    axes[0].bar(dist_values, dist_counts, edgecolor='black', alpha=0.7)
    axes[0].axvline(avg_path_length, color='red', linestyle='--',
                   label=f'Average: {avg_path_length:.2f}')
    axes[0].axvline(effective_diameter, color='green', linestyle='--',
                   label=f'Eff. diameter: {effective_diameter}')
    axes[0].set_xlabel('Distance')
    axes[0].set_ylabel('Frequency')
    axes[0].set_title('Shortest Path Distance Distribution')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    # CDF Plot (Cumulative Distance)
    total_pairs = sum(dist_counts)
    cdf_values = np.cumsum(dist_counts) / total_pairs

    axes[1].plot(dist_values, cdf_values, 'bo-', linewidth=2, markersize=4)
    axes[1].axhline(0.9, color='green', linestyle='--', alpha=0.5)
    axes[1].axvline(effective_diameter, color='green', linestyle='--', alpha=0.5)
    axes[1].set_xlabel('Distance')
    axes[1].set_ylabel('Fraction of Node Pairs')
    axes[1].set_title('Cumulative Distance (CDF)')
    axes[1].grid(True, alpha=0.3)
    axes[1].set_ylim(0, 1.05)

    plt.tight_layout()
    plt.show()

    # Print results
    print(f"\n=== DISTANCE RESULTS ===")
    print(f"Average path length: {avg_path_length:.4f}")
    print(f"Diameter (maximum distance): {max_distance}")
    print(f"Effective diameter (90%): {effective_diameter}")
    print(f"Number of node pairs: {len(all_distances)}")
    print(f"Most common distance: {distances_counter.most_common(1)[0][0]} " +
          f"(appears {distances_counter.most_common(1)[0][1]} times)")

    # Return metrics
    return {
        'avg_path_length': avg_path_length,
        'diameter': max_distance,
        'effective_diameter': effective_diameter,
        'distance_distribution': distances_counter,
        'all_distances': all_distances
    }

"""**Simple plot of degree distribution**"""

# ============================================
# 4. Degrees distribution
# ============================================

def plot_distribucion_grado(G):
    """
    Generates plots of the degree distribution
    """
    # Calcular grados
    grados = [d for n, d in G.degree()]
    grado_promedio = np.mean(grados)
    grado_max = np.max(grados)

    # Crear figura con subplots
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))

    # 1. Histograma de grados
    axes[0].hist(grados, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
    axes[0].axvline(grado_promedio, color='red', linestyle='--',
                   label=f'Average: {grado_promedio:.2f}')
    axes[0].set_xlabel('Degree')
    axes[0].set_ylabel('Frequency')
    axes[0].set_title('Degree Distribution')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    # 2. DistribuciÃ³n acumulativa
    grados_ordenados = np.sort(grados)
    probabilidad_acumulada = np.arange(1, len(grados_ordenados) + 1) / len(grados_ordenados)

    axes[1].plot(grados_ordenados, probabilidad_acumulada, 'b-', linewidth=2)
    axes[1].set_xlabel('Degree')
    axes[1].set_ylabel('Cumulative Probability')
    axes[1].set_title('Cumulative Degree Distribution')
    axes[1].grid(True, alpha=0.3)

    # 3. Top 10 nodos con mayor grado
    top_grados = sorted(G.degree(), key=lambda x: x[1], reverse=True)[:10]
    nodos_top = [n[0] for n in top_grados]
    valores_top = [n[1] for n in top_grados]

    axes[2].barh(nodos_top, valores_top, color='lightcoral')
    axes[2].set_xlabel('Degree')
    axes[2].set_title('Top 10 Nodes with the Highest Degree')
    axes[2].invert_yaxis()  # El mayor grado en la parte superior

    plt.tight_layout()
    plt.savefig('distribucion_grado.png', dpi=300, bbox_inches='tight')
    plt.show()

    # EstadÃ­sticas adicionales
    print("\n=== DEGREE STATISTICS===")
    print(f"Average degree: {grado_promedio:.2f}")
    print(f"Highest degree: {grado_max}")
    print(f"Minimum degree: {np.min(grados)}")
    print(f"Standard deviation: {np.std(grados):.2f}")

    # DistribuciÃ³n de grados por residuo
    if G.number_of_nodes() > 0:
        grados_por_residuo = {}
        for nodo, attr in G.nodes(data=True):
            resn = attr.get('resn', 'UNK')
            if resn not in grados_por_residuo:
                grados_por_residuo[resn] = []
            grados_por_residuo[resn].append(G.degree(nodo))

        # Calcular promedio por tipo de residuo
        promedio_por_residuo = {resn: np.mean(grados) for resn, grados in grados_por_residuo.items()}

        # Top 10 residuos con mayor grado promedio
        top_residuos = sorted(promedio_por_residuo.items(), key=lambda x: x[1], reverse=True)[:10]
        print("\nTop 10 residues with a higher average grade:")
        for resn, avg_degree in top_residuos:
            print(f"  {resn}: {avg_degree:.2f}")

    return grados

"""**Network topology analysis**

Analyzing network topology by focusing on how the connections are distributed.
- The PDF (Probability Density Function) will show us the probability of finding a node with a specific degree k
- The CCDF (Complementary Cumulative Distribution Function) identify whether the network is "scale-free".
"""

import powerlaw

def analyze_degree_statistics(G):
    # 1. Get degree sequence
    degrees = [d for n, d in G.degree()]

    # 2. Distribution fitting with powerlaw
    # The xmin parameter indicates where the tail evaluation starts
    fit = powerlaw.Fit(degrees, discrete=True)

    # --- VISUALIZATION ---
    fig, ax = plt.subplots(1, 2, figsize=(14, 6))

    # A. PDF plot (usually in log-log scale to see behavior)
    # Use logarithmic bins to reduce noise
    powerlaw.plot_pdf(degrees, ax=ax[0], color='b', marker='o', linestyle='None', label='Data (PDF)')
    ax[0].set_title("Degree PDF (Log-Log)")
    ax[0].set_xlabel("Degree (k)")
    ax[0].set_ylabel("P(k)")

    # B. CCDF plot and comparison of fits
    powerlaw.plot_ccdf(degrees, ax=ax[1], color='black', linewidth=2, label='Data (CCDF)')

    # Draw candidate fits
    fit.power_law.plot_ccdf(ax=ax[1], color='r', linestyle='--', label='Power Law Fit')
    fit.lognormal.plot_ccdf(ax=ax[1], color='g', linestyle='--', label='Log-Normal Fit')
    fit.exponential.plot_ccdf(ax=ax[1], color='orange', linestyle='--', label='Exponential Fit')

    ax[1].set_title("Degree CCDF and Candidate Fits")
    ax[1].set_xlabel("Degree (k)")
    ax[1].set_ylabel("P(K â‰¥ k)")
    ax[1].legend()

    plt.tight_layout()
    plt.show()

    # --- GOODNESS OF FIT COMPARISON ---
    print(f"Estimated Alpha value (Power Law): {fit.power_law.alpha:.2f}")
    print(f"xmin value (tail start): {fit.xmin}")

    # Compare Power Law vs other distributions
    # R is the log-likelihood ratio. If R > 0, the first model is better.
    # p is the significance value.

    R, p = fit.distribution_compare('power_law', 'exponential')
    print(f"\nPower Law vs Exponential comparison: R={R:.2f}, p-value={p:.4f}")

    R_log, p_log = fit.distribution_compare('power_law', 'lognormal')
    print(f"Power Law vs Log-Normal comparison: R={R_log:.2f}, p-value={p_log:.4f}")

# Execution:
# analyze_degree_statistics(G)

"""#### b) Centrality analysis"""

# ============================================
# 6. ANÃLISIS DE CENTRALIDAD
# ============================================
def analyze_centrality(G):
    """
    Calculates and visualizes centrality measures
    """
    print("\n=== CENTRALITY ANALYSIS ===")

    try:
        # Calculate different centrality measures
        print("Calculating betweenness centrality...")
        betweenness = nx.betweenness_centrality(G, normalized=True)

        print("Calculating closeness centrality...")
        closeness = nx.closeness_centrality(G)

        print("Calculating degree centrality...")
        degree_centrality = nx.degree_centrality(G)

        print("Calculating eigenvector centrality...")
        eigenvector = nx.eigenvector_centrality(G, max_iter=1000, tol=1e-6)

        # Create DataFrame with all measures
        centrality_df = pd.DataFrame({
            'Node': list(G.nodes()),
            'Betweenness': [betweenness[n] for n in G.nodes()],
            'Closeness': [closeness[n] for n in G.nodes()],
            'Degree_Centrality': [degree_centrality[n] for n in G.nodes()],
            'Eigenvector': [eigenvector[n] for n in G.nodes()]
        })

        # Add residue information
        residue_info = []
        for node in G.nodes():
            attr = G.nodes[node]
            residue_info.append(f"{attr.get('chain', '')}/{attr.get('resi', '')}/{attr.get('resn', '')}")

        centrality_df['Residue'] = residue_info

        # Centrality histograms
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        axes = axes.flatten()
        measures = ['Betweenness', 'Closeness', 'Degree_Centrality', 'Eigenvector']
        colors = ['lightblue', 'lightgreen', 'lightcoral', 'plum']

        for idx, (measure, color) in enumerate(zip(measures, colors)):
            axes[idx].hist(centrality_df[measure], bins=20, alpha=0.7,
                          color=color, edgecolor='black')
            axes[idx].set_xlabel(f'{measure} Value')
            axes[idx].set_ylabel('Frequency')
            axes[idx].set_title(f'Distribution of {measure}')
            axes[idx].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig('centrality_histograms.png', dpi=300, bbox_inches='tight')
        plt.show()

        # Top nodes for each centrality measure
        print("\n=== TOP NODES BY CENTRALITY ===")

        for measure in measures:
            top_nodes = centrality_df.nlargest(5, measure)[['Node', 'Residue', measure]]
            print(f"\nTop 5 by {measure}:")
            print(top_nodes.to_string(index=False))

        return centrality_df

    except Exception as e:
        print(f"Error calculating centrality: {e}")
        print("Trying with basic measures...")

        # Only degree centrality if others fail
        degree_centrality = nx.degree_centrality(G)

        centrality_df = pd.DataFrame({
            'Node': list(G.nodes()),
            'Degree_Centrality': [degree_centrality[n] for n in G.nodes()]
        })

        return centrality_df

"""**Centrality structure: Rank the top 50 nodes**"""

def analyze_centrality_rankings(G, top_n=50):
    print("Calculating centrality metrics... this may take a moment.")

    # 1. Centrality Calculations
    dict_centralities = {
        'Degree': nx.degree_centrality(G),
        'Closeness': nx.closeness_centrality(G),
        'Betweenness': nx.betweenness_centrality(G),
        'Eigenvector': nx.eigenvector_centrality(G, max_iter=1000),
        'Katz': nx.katz_centrality(G, alpha=0.1, beta=1.0),
        'PageRank': nx.pagerank(G),
        'Subgraph': nx.subgraph_centrality(G)
    }

    # 2. Create DataFrame to compare rankings
    df_ranks = pd.DataFrame(index=G.nodes())

    for name, values in dict_centralities.items():
        # Save the value
        df_ranks[f'{name}_Val'] = pd.Series(values)
        # Create ranking (1 = most central)
        df_ranks[f'{name}_Rank'] = df_ranks[f'{name}_Val'].rank(ascending=False, method='min')

    # 3. Get Top 50 for each metric
    top_nodes_dict = {}
    for name in dict_centralities.keys():
        top_list = df_ranks.sort_values(by=f'{name}_Val', ascending=False).head(top_n).index.tolist()
        top_nodes_dict[name] = top_list

    df_top_50 = pd.DataFrame(top_nodes_dict)

    # 4. Export results
    df_top_50.to_csv("top_50_centralities.csv", index=False)
    print("Top 50 saved to 'top_50_centralities.csv'")

    return df_ranks, df_top_50

# Execution
# df_full, df_top50 = analyze_centrality_rankings(G)

import py3Dmol

def visualize_persistence_3d(df_key_residues, pdb_path='pdb_path'):
#def visualize_persistence_3d(df_key_residues, pdb_id='6EDW'):
    # 1. Create viewer and load protein from PDB
    ##view = py3Dmol.view(query=f'pdb:{pdb_id}')

    # Upload from local archive
    view = py3Dmol.view()
    view.addModel(open(pdb_path, 'r').read(), 'pdb')

    # 2. Default style: Gray semi-transparent cartoon
    view.setStyle({'cartoon': {'color': '#e0e0e0', 'opacity': 0.6}})

    # 3. Define function to assign colors based on percentage
    # Use Blue (low) to Red (high) scale
    def get_color(percentage):
        if percentage == 100: return '#FF0000' # Red (Maximum consensus)
        if percentage >= 70:  return '#FF8C00' # Orange
        if percentage >= 50:  return '#FFFF00' # Yellow
        if percentage >= 30:  return '#00FF00' # Green
        return '#0000FF' # Blue (Specialists)

    # 4. Color Top residues according to their persistence
    # Node format is D/385/GLU -> [Chain, Number, AA]
    for _, row in df_key_residues.iterrows():
        parts = row['Node'].split('/')
        chain = parts[0]
        number = int(parts[1])
        percentage = row['Percentage']

        # Select specific residue
        selection = {'chain': chain, 'resi': number}

        # Apply style: Spheres (VDW) to highlight key residues
        color_hex = get_color(percentage)
        view.addStyle(selection, {'stick': {'colorscheme': f'{color_hex}raw', 'radius': 0.3}})
        view.addStyle(selection, {'sphere': {'colorscheme': f'{color_hex}raw', 'scale': 0.8}})

        # Optional: Add hover label
        view.addLabel(f"{parts[2]} {number} ({int(percentage)}%)",
                      {'fontSize': 10, 'fontColor': 'black', 'backgroundColor': 'white', 'backgroundOpacity': 0.5},
                      selection)

    # 5. Center and display
    view.zoomTo()
    #print(f"Visualizing structure {pdb_path}")
    print(f"Visualizing structure {pdb_path}")
    print("ðŸ”´ Red: 100% | ðŸŸ  Orange: >70% | ðŸŸ¡ Yellow: >50% | ðŸ”µ Blue: <30%")
    return view.show()

# Execution:
# visualize_persistence_3d(df_key_residues, pdb_id='6EDW')
# visualize_persistence_3d(df_key_residues, pdb_path='pdb_path')

def visualize_persistence_3d_clean(df_key_residues, pdb_path='pdb_path'):
    # 1. Create the viewer
    #view = py3Dmol.view(query=f'pdb:{pdb_id}')
       #  view = py3Dmol.view(query=f'pdb:{pdb_id}')
    view = py3Dmol.view()
    view.removeAllModels() # Clean any residue from previous loading
    view.addModel(open(pdb_path, 'r').read(), 'pdb')

    # 2. Base style for entire protein (Light Gray)
    view.setStyle({'cartoon': {'color': '#D3D3D3', 'opacity': 0.7}})

    # Identify Top 10 for labels
    top_10_nodes = df_key_residues.head(10)['Node'].tolist()

    # 3. Define solid color scale
    def get_color_hex(percentage):
        if percentage == 100: return '0xFF0000' # Red
        if percentage >= 75:  return '0xFFA500' # Orange
        if percentage >= 50:  return '0xFFFF00' # Yellow
        return '0x0000FF' # Blue

    # 4. Iterate over ranking residues
    for idx, row in df_key_residues.iterrows():
        node = row['Node']
        parts = node.split('/')
        chain = parts[0]
        res_num = int(parts[1])
        aa_name = parts[2]
        percentage = row['Percentage']

        col_hex = get_color_hex(percentage)
        selection = {'chain': chain, 'resi': res_num}

        # Apply Stick and Sphere with forced color (no CPK)
        # Use 'color' instead of 'colorscheme' to avoid atomic standard
        view.addStyle(selection, {
            'stick': {'color': col_hex, 'radius': 0.2},
            'sphere': {'color': col_hex, 'scale': 0.7}
        })

        # 5. LABELS ONLY FOR TOP 10
        if node in top_10_nodes:
            view.addLabel(f"{aa_name}{res_num} ({int(percentage)}%)",
                          {
                              'fontSize': 12,
                              'fontColor': 'white',
                              'backgroundColor': col_hex,
                              'backgroundOpacity': 0.8,
                              'showBackground': True
                          },
                          selection)

    view.zoomTo()
    print(f"âœ… Visualizing {pdb_path}. Labels only shown for the 10 most persistent residues.")
    return view.show()

# Execution
# visualize_persistence_3d_clean(df_key_residues, pdb_id='6EDW')

import py3Dmol

def visualize_3d_persistence_pro(df_key_residues, pdb_path='pdb_path', show_labels=True):
    """
    Visualizes the protein coloring residues by persistence.
    - show_labels: True to view Top 10, False for a clean view.
    """
    # 1. Viewer configuration
   #  view = py3Dmol.view(query=f'pdb:{pdb_id}')
    view = py3Dmol.view()
    view.removeAllModels() # Clean any residue from previous loading
    view.addModel(open(pdb_path, 'r').read(), 'pdb')

    # 2. Base style (Gray Cartoon)
    view.setStyle({'cartoon': {'color': '#D3D3D3', 'opacity': 0.6}})

    # Color scale (Clean hexadecimals)
    colors_map = {
        100: '#FF0000', # Red
        75:  '#FFA500', # Orange
        50:  '#FFFF00', # Yellow
        0:   '#0000FF'  # Blue (for the rest of the top)
    }

    top_10_nodes = df_key_residues.head(10)['Node'].tolist()

    # 3. Residue mapping
    for idx, row in df_key_residues.iterrows():
        parts = row['Node'].split('/')
        chain = parts[0]
        res_num = int(parts[1])
        aa_name = parts[2]
        percentage = row['Percentage']

        # Determine color based on range
        if percentage == 100: color = colors_map[100]
        elif percentage >= 75: color = colors_map[75]
        elif percentage >= 50: color = colors_map[50]
        else: color = colors_map[0]

        selection = {'chain': chain, 'resi': res_num}

        # Force solid color in Stick and Sphere
        view.addStyle(selection, {
            'stick': {'color': color, 'radius': 0.25},
            'sphere': {'color': color, 'scale': 0.8}
        })

        # 4. Label logic
        if show_labels and (row['Node'] in top_10_nodes):
            view.addLabel(f"{aa_name}{res_num}",
                          {
                              'fontSize': 10,
                              'fontColor': 'black',
                              'backgroundColor': 'white',
                              'backgroundOpacity': 0.6
                          },
                          selection)

    view.zoomTo()
    # Bonus: Add stronger ambient light so colors stand out
    view.setClickable(True)

    status = "with labels (Top 10)" if show_labels else "without labels (clean view)"
    print(f"ðŸ”¹ Showing {pdb_path} {status}")

    return view.show()

# --- EXECUTION MODES ---

# Option A: Clean view (Only colors)
# visualize_3d_persistence_pro(df_key_residues, pdb_path='pdb_path', show_labels=False)

# Option B: View with labels
# visualize_3d_persistence_pro(df_key_residues, pdb_path='pdb_path', show_labels=True)

"""#### c) Community analysis

**Infomap Community analysis**
"""

from collections import Counter
from infomap import Infomap

def analyze_and_visualize_infomap(G, csv_name="community_results.csv"):
    # 1. Infomap execution
    im = Infomap("--silent")
    mapping = im.add_networkx_graph(G)
    im.run()

    # 2. Community dictionary and CSV export
    comm_dict = {mapping[node.node_id]: node.module_id for node in im.tree if node.is_leaf}
    pd.DataFrame(list(comm_dict.items()), columns=['Node', 'Community']).to_csv(csv_name, index=False)

    # 3. Identify the 10 largest communities
    count = Counter(comm_dict.values())
    top_10_ids = [community for community, _ in count.most_common(10)]

    # 4. Color assignment
    # Use a color map for top 10 and gray for the rest
    cmap = plt.cm.get_cmap('tab10', 10)
    color_map_top = {com_id: cmap(i) for i, com_id in enumerate(top_10_ids)}

    # List of colors for each node in G
    node_colors = [
        color_map_top[comm_dict[node]] if comm_dict[node] in top_10_ids else (0.9, 0.9, 0.9)
        for node in G.nodes()
    ]

    # 5. Visualization
    pos = nx.spring_layout(G, seed=42)
    plt.figure(figsize=(12, 8))

    # Draw edges with high transparency
    nx.draw_networkx_edges(G, pos, alpha=0.1, edge_color='gray')

    # Draw nodes
    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=100)

    # 6. Create Custom Legend (only Top 10)
    legend_handles = [
        plt.Line2D([0], [0], marker='o', color='w', label=f'Community {cid} ({count[cid]} nodes)',
                   markerfacecolor=color_map_top[cid], markersize=10)
        for cid in top_10_ids
    ]

    plt.legend(handles=legend_handles, title="Top 10 Communities", loc='center left', bbox_to_anchor=(1, 0.5))
    plt.title(f"Infomap: Focus on the 10 Main Communities\n(Total: {len(count)} communities)", fontsize=14)
    plt.axis('off')
    plt.tight_layout()
    plt.show()

    return [[n for n, c in comm_dict.items() if c == i] for i in range(1, max(comm_dict.values()) + 1)]

"""**comparative_community_analysis**"""

from collections import Counter
from infomap import Infomap

def comparative_community_analysis(G, csv_file_name="community_comparison.csv"):
    # Initialize DataFrame with graph nodes
    nodes = list(G.nodes())
    df_final = pd.DataFrame({'Node': nodes})

    # Define algorithms to execute
    # Each entry is: (Name, Algorithm_Function)

    # 1. Infomap (separate logic as it's an external library)
    im = Infomap("--silent")
    mapping = im.add_networkx_graph(G)
    im.run()
    infomap_dict = {mapping[node.node_id]: node.module_id for node in im.tree if node.is_leaf}
    df_final['Infomap'] = df_final['Node'].map(infomap_dict)

    # 2. Dictionary of NetworkX algorithms
    # Note: K-Clique may leave nodes without community, we'll mark them as 0
    methods = {
        'Modularity': lambda: nx.community.greedy_modularity_communities(G, weight='weight'),
        'Label_Propagation': lambda: nx.community.label_propagation_communities(G),
        'Girvan_Newman': lambda: next(nx.community.girvan_newman(G)), # Take first level
        'K_Clique_3': lambda: nx.community.k_clique_communities(G, 3)
    }

    # Execution and visualization
    for name, func in methods.items():
        try:
            community_sets = list(func())
            # Convert list of sets to dictionary {node: community_id}
            temp_dict = {}
            for i, cluster in enumerate(community_sets, 1):
                for node in cluster:
                    temp_dict[node] = i

            # Save to DataFrame (nodes not assigned in K-Clique remain as NaN/0)
            df_final[name] = df_final['Node'].map(temp_dict).fillna(0).astype(int)

            # Plot
            _plot_top_communities(G, temp_dict, name)

        except StopIteration:
            print(f"Error processing {name}")

    # Save the unified CSV
    df_final.to_csv(csv_file_name, index=False)
    print(f"âœ… Analysis complete. File saved as: {csv_file_name}")

    # Also plot Infomap which was processed outside the loop
    _plot_top_communities(G, infomap_dict, "Infomap")

    return df_final

def _plot_top_communities(G, comm_dict, title):
    """Internal function to maintain consistency in plots"""
    count = Counter(comm_dict.values())
    top_10_ids = [community for community, _ in count.most_common(10)]

    cmap = plt.cm.get_cmap('tab10', 10)
    color_map_top = {com_id: cmap(i) for i, com_id in enumerate(top_10_ids)}

    node_colors = [
        color_map_top[comm_dict[node]] if node in comm_dict and comm_dict[node] in top_10_ids
        else (0.9, 0.9, 0.9) for node in G.nodes()
    ]

    pos = nx.spring_layout(G, seed=42)
    plt.figure(figsize=(10, 6))
    nx.draw_networkx_edges(G, pos, alpha=0.1, edge_color='gray')
    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=100)

    # Legend
    legend_handles = [
        plt.Line2D([0], [0], marker='o', color='w', label=f'C{cid} ({count[cid]} n.)',
                   markerfacecolor=color_map_top[cid], markersize=8)
        for cid in top_10_ids
    ]

    plt.legend(handles=legend_handles, title="Top 10", loc='center left', bbox_to_anchor=(1, 0.5))
    plt.title(f"Algorithm: {title}", fontsize=14)
    plt.axis('off')
    plt.tight_layout()
    plt.show()

import matplotlib.colors as mcolors

def visualize_local_pdb_communities(df_communities, method, pdb_path, n_communities=5):
    """
    Visualizes local PDB file coloring the N largest communities.

    Inputs:
    - df_communities: DataFrame (df_community_analisis_G_S01)
    - method: String ('Infomap', 'Modularity', etc.)
    - pdb_path: Local path to the .pdb file
    - n_communities: Number of largest communities to color
    """

    # 1. Identify the N largest communities for this method
    top_comms = df_communities[method].value_counts().nlargest(n_communities).index.tolist()

    # 2. Prepare color palette (using a Matplotlib colormap)
    cmap = plt.get_cmap('tab10') # Palette with 10 distinct colors
    hex_colors = [mcolors.to_hex(cmap(i)) for i in range(n_communities)]
    color_map = dict(zip(top_comms, hex_colors))

    # 3. Read the local PDB file
    try:
        with open(pdb_path, 'r') as f:
            pdb_data = f.read()
    except FileNotFoundError:
        print(f"âŒ Error: File not found at {pdb_path}")
        return

    # 4. Configure the viewer
    view = py3Dmol.view(width=800, height=600)
    view.addModel(pdb_data, 'pdb')

    # Base style: Transparent cartoon for non-top communities
    view.setStyle({'cartoon': {'color': '#f0f0f0', 'opacity': 0.2}})

    # 5. Color each community
    print(f"ðŸ“Š Coloring the {n_communities} largest communities for method: {method}")

    for i, comm_id in enumerate(top_comms):
        color = color_map[comm_id]
        # Filter nodes belonging to this community
        comm_nodes = df_communities[df_communities[method] == comm_id]['Node'].tolist()

        for node in comm_nodes:
            parts = node.split('/')
            chain = parts[0]
            res_num = int(parts[1])

            selection = {'chain': chain, 'resi': res_num}
            # Apply color to the community
            view.addStyle(selection, {'cartoon': {'color': color, 'opacity': 1.0}})

        print(f"   - Community {comm_id}: {color} ({len(comm_nodes)} residues)")

    view.zoomTo()
    return view.show()

# --- USAGE EXAMPLE ---
# path = "C:/users/documents/6EDW.pdb"  # Adjust your path
# visualize_local_pdb_communities(df_community_analisis_G_S01, 'Infomap', path, n_communities=4)

"""#### **Consensus analysis between community analysis algorithms**"""

def generate_consensus_matrix(df):
    # Extract only algorithm columns
    matrix_data = df.iloc[:, 1:].values
    n_nodes = matrix_data.shape[0]
    n_algorithms = matrix_data.shape[1]

    # Initialize co-occurrence matrix
    consensus = np.zeros((n_nodes, n_nodes))

    # Sum matches for each algorithm
    for i in range(n_algorithms):
        col = matrix_data[:, i].reshape(-1, 1)
        # Creates a boolean matrix where True means they belong to the same community
        consensus += (col == col.T)

    # Normalize (0 to 1) where 1 is total consensus across all methods
    consensus_norm = consensus / n_algorithms
    return consensus_norm



def extract_consensus_and_plot(df_analysis, threshold=1.0):
    """
    Analyzes node co-occurrence in communities across multiple methods.

    Inputs:
    - df_analysis: DataFrame with columns ['Node', 'Method1', 'Method2', ...]
    - threshold: Value from 0 to 1. 1.0 means nodes must match in ALL methods.
    """
    # --- 1. Preparation and Co-occurrence Calculation ---
    nodes = df_analysis['Node'].values
    matrix_data = df_analysis.iloc[:, 1:].values
    n_nodes = matrix_data.shape[0]
    n_algorithms = matrix_data.shape[1]

    # Vectorized calculation for better speed
    consensus = np.zeros((n_nodes, n_nodes))
    for i in range(n_algorithms):
        col = matrix_data[:, i].reshape(-1, 1)
        consensus += (col == col.T)

    consensus_norm = consensus / n_algorithms

    # --- 2. Extraction of Strong Relationships (Step 2) ---
    relationships = []
    # Use triu (upper triangle) to avoid duplicate pairs (A,B) and (B,A)
    rows, cols = np.where(np.triu(consensus_norm, k=1) >= threshold)

    for r, c in zip(rows, cols):
        relationships.append({
            'Node_A': nodes[r],
            'Node_B': nodes[c],
            'Consensus_Strength': consensus_norm[r, c]
        })

    df_consensus = pd.DataFrame(relationships)

    # --- 3. Generation of Consensus Graph (Step 3) ---
    G_cons = nx.Graph()
    if not df_consensus.empty:
        for _, row in df_consensus.iterrows():
            G_cons.add_edge(row['Node_A'], row['Node_B'], weight=row['Consensus_Strength'])

    # Visualization
    plt.figure(figsize=(12, 10))
    if len(G_cons) > 0:
        pos = nx.spring_layout(G_cons, k=0.15, seed=42)
        # Draw with clean style
        nx.draw_networkx_edges(G_cons, pos, alpha=0.2, edge_color='royalblue')
        nx.draw_networkx_nodes(G_cons, pos, node_size=30, node_color='darkblue', alpha=0.7)

        # Optional: Labels only if there are few nodes
        if len(G_cons) < 50:
            nx.draw_networkx_labels(G_cons, pos, font_size=8)

        plt.title(f"Consensus Graph (Threshold >= {threshold})\nNodes that algorithms consistently group together")
    else:
        plt.text(0.5, 0.5, "No relationships found with this threshold",
                 ha='center', va='center', fontsize=12)

    plt.axis('off')
    plt.show()

    print(f"Identified {len(df_consensus)} reliable relationships.")
    return df_consensus, G_cons

def plot_unit_count(df_multithreshold):
    """
    This function takes the df_multithreshold generated by the extract_consensus_and_plot function
    and summarizes the network complexity at each level.
    """
    # Count unique values per column (excluding 'Node')
    thresholds = df_multithreshold.columns[1:]
    counts = [df_multithreshold[u].nunique() for u in thresholds]

    plt.figure(figsize=(10, 6))
    bars = plt.bar(thresholds, counts, color='skyblue', edgecolor='navy')
    plt.bar_label(bars, padding=3)
    plt.title("Evolution of the number of communities according to consensus threshold")
    plt.ylabel("Number of Communities")
    plt.xlabel("Consensus Level (Threshold)")
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.show()



def generate_multi_threshold_consensus_csv(df_analysis, thresholds=[1.0, 0.9, 0.8, 0.7, 0.6], output_name="multi_threshold_consensus.csv"):
    """
    Takes the algorithm comparison dataframe and creates a table of
    communities based on consensus for different rigor levels.
    """
    # 1. Extract data and prepare co-occurrence matrix
    nodes = df_analysis['Node'].values
    matrix_data = df_analysis.iloc[:, 1:].values
    n_nodes = matrix_data.shape[0]
    n_methods = matrix_data.shape[1]

    # Calculate how many times each node pair coincides in the same cluster
    consensus = np.zeros((n_nodes, n_nodes))
    for i in range(n_methods):
        col = matrix_data[:, i].reshape(-1, 1)
        consensus += (col == col.T)

    # Normalize (from 0.0 to 1.0)
    consensus_norm = consensus / n_methods

    # 2. Create output DataFrame
    df_multi_threshold = pd.DataFrame({'Node': nodes})

    # 3. For each threshold, find connected components (communities)
    for u in thresholds:
        G_temp = nx.Graph()
        G_temp.add_nodes_from(nodes)

        # Only create edges between nodes that coincide in at least 'u'% of cases
        rows, cols = np.where(np.triu(consensus_norm, k=1) >= u)
        for f, c in zip(rows, cols):
            G_temp.add_edge(nodes[f], nodes[c])

        # Extract resulting communities
        components = list(nx.connected_components(G_temp))

        # Map each node to its community ID at this specific threshold
        mapping = {}
        for idx, cluster in enumerate(components, 1):
            for node in cluster:
                mapping[node] = idx

        df_multi_threshold[f'threshold_{u}'] = df_multi_threshold['Node'].map(mapping)

    # 4. Save file
    df_multi_threshold.to_csv(output_name, index=False)
    print(f"Multi-threshold consensus file generated: {output_name}")

    return df_multi_threshold

import plotly.graph_objects as go

def generate_filtered_sankey(df_multi_threshold, min_nodes=5):
    """
    Generates an interactive Sankey Diagram showing how nodes are grouped
    across different consensus thresholds.

    Inputs:
    - df_multi_threshold: DataFrame with columns ['Node', 'threshold_1.0', 'threshold_0.9', ...]
    - min_nodes: Communities with fewer than this number of nodes are grouped into 'Others'.
    """
    # 1. Prepare threshold column names
    threshold_cols = [c for c in df_multi_threshold.columns if c.startswith('threshold_')]
    df_plot = df_multi_threshold.copy()

    # 2. Filter noise: Group small communities
    for col in threshold_cols:
        counts = df_plot[col].value_counts()
        small = counts[counts < min_nodes].index
        # Mark as -1 communities that don't reach the minimum
        df_plot.loc[df_plot[col].isin(small), col] = -1

    # 3. Build Nodes and Flows for Plotly
    labels = []
    nodes_idx_map = {}

    # Create unique labels for each community at each level
    for col in threshold_cols:
        u_val = col.split('_')[1]
        unique_comms = sorted(df_plot[col].unique())
        for comm in unique_comms:
            node_name = f"{col}_C{comm}"
            nodes_idx_map[node_name] = len(labels)
            if comm == -1:
                labels.append(f"U{u_val} Miscellaneous")
            else:
                labels.append(f"U{u_val} C{comm}")

    sources, targets, values = [], [], []

    # Create connections between consecutive levels
    for i in range(len(threshold_cols) - 1):
        current = threshold_cols[i]
        next_col = threshold_cols[i+1]

        # Group flows
        flows = df_plot.groupby([current, next_col]).size().reset_index(name='count')

        for _, row in flows.iterrows():
            sources.append(nodes_idx_map[f"{current}_C{row[current]}"])
            targets.append(nodes_idx_map[f"{next_col}_C{row[next_col]}"])
            values.append(row['count'])

    # 4. Generate interactive chart
    fig = go.Figure(data=[go.Sankey(
        node = dict(
            pad = 15, thickness = 20,
            line = dict(color = "black", width = 0.5),
            label = labels,
            color = "royalblue"
        ),
        link = dict(
            source = sources,
            target = targets,
            value = values,
            color = "rgba(173, 216, 230, 0.4)" # Light blue transparent
        )
    )])

    fig.update_layout(
        title_text=f"Sankey Diagram: Community Stability (Minimum {min_nodes} nodes)",
        font_size=12,
        height=800
    )

    fig.show(renderer="notebook")

# --- HOW TO EXECUTE THE ENTIRE FLOW ---

# 1. Generate the multi-threshold dataframe (using the function I gave you earlier)
# df_multi_threshold = generate_multi_threshold_consensus_csv(df_analysis, thresholds=[1.0, 0.9, 0.8, 0.7, 0.6])

# 2. Execute Sankey passing that specific dataframe
# generate_filtered_sankey(df_multi_threshold, min_nodes=5)

"""#### Null models"""

import powerlaw
import warnings
from scipy.optimize import OptimizeWarning
def calculate_network_metrics(G):
    """Calculates metrics for items 2-5 and 7 for a given network."""
    # Degrees for PDF/CCDF and Alpha
    degrees = [d for n, d in G.degree() if d > 0]
    # Suprimir warnings especÃ­ficos de powerlaw
    with warnings.catch_warnings():
        warnings.filterwarnings('ignore', category=OptimizeWarning)
        warnings.filterwarnings('ignore', category=FutureWarning)
        fit = powerlaw.Fit(degrees, discrete=True, verbose=False)
        alpha = fit.power_law.alpha

    # Centralities (averages)
    # Note: We use averages to be able to compare groups of networks
    bet = np.mean(list(nx.betweenness_centrality(G).values()))
    clo = np.mean(list(nx.closeness_centrality(G).values()))
    eig = np.mean(list(nx.eigenvector_centrality(G, max_iter=1000).values()))
    pagerank = np.mean(list(nx.pagerank(G).values()))

    return {
        'Alpha': alpha,
        'Avg_Betweenness': bet,
        'Avg_Closeness': clo,
        'Avg_Eigenvector': eig,
        'Avg_PageRank': pagerank
    }

def null_models_analysis(G_real):
    n = G_real.number_of_nodes()
    m = G_real.number_of_edges()

    # Real Network Metrics
    res_real = calculate_network_metrics(G_real)

    er_results = []
    ba_results = []

    for i in range(10):
        # ErdÅ‘sâ€“RÃ©nyi: same n and m
        G_er = nx.gnm_random_graph(n, m)
        er_results.append(calculate_network_metrics(G_er))

        # BarabÃ¡siâ€“Albert: same n and m (m_ba is approx m/n)
        m_ba = max(1, int(m/n))
        G_ba = nx.barabasi_albert_graph(n, m_ba)
        # Adjust edges exactly if needed, but BA is structural
        ba_results.append(calculate_network_metrics(G_ba))

    # Tabulate results
    df_er = pd.DataFrame(er_results)
    df_ba = pd.DataFrame(ba_results)

    comparison = {
        'Metric': res_real.keys(),
        'Real': res_real.values(),
        'ER_Mean': df_er.mean(),
        'ER_Std': df_er.std(),
        'BA_Mean': df_ba.mean(),
        'BA_Std': df_ba.std()
    }

    return pd.DataFrame(comparison)

# Execution
# df_nulls = null_models_analysis(G)
# print(df_nulls)

def plot_null_model_comparison(G_real, n_simulations=10):
    n = G_real.number_of_nodes()
    m = G_real.number_of_edges()

    # 1. Get real network metrics
    real_metrics = calculate_network_metrics(G_real)
    metric_names = list(real_metrics.keys())

    # 2. Simulate networks and collect data
    data_list = []
    m_ba = max(1, int(m/n))

    for i in range(n_simulations):
        # ER
        G_er = nx.gnm_random_graph(n, m)
        metrics_er = calculate_network_metrics(G_er)
        for name in metric_names:
            data_list.append({'Metric': name, 'Value': metrics_er[name], 'Type': 'ER (Random)'})

        # BA
        G_ba = nx.barabasi_albert_graph(n, m_ba)
        metrics_ba = calculate_network_metrics(G_ba)
        for name in metric_names:
            data_list.append({'Metric': name, 'Value': metrics_ba[name], 'Type': 'BA (Scale-free)'})

    df_simulations = pd.DataFrame(data_list)

    # 3. Create subplots (one per metric)
    fig, axes = plt.subplots(1, len(metric_names), figsize=(20, 6))

    for i, name in enumerate(metric_names):
        # Filter data for current metric
        df_sub = df_simulations[df_simulations['Metric'] == name]

        # Draw Boxplot of null models
        sns.boxplot(
            data=df_sub,
            x='Type',
            y='Value',
            hue='Type',
            palette='Pastel1',
            width=0.5,
            ax=axes[i],
            legend=False
        )

        # Overlay real value as a prominent red point
        axes[i].plot(0.5, real_metrics[name], marker='D', color='red', markersize=10,
                     label='Real Network (Protein)', linestyle='None')

        axes[i].set_title(f'{name}', fontsize=12)
        axes[i].set_xlabel('')
        axes[i].set_ylabel('Value')
        if i == 0:
            axes[i].legend()

    plt.suptitle('Comparison: Protein Network vs. Null Models (ER and BA)', fontsize=16, y=1.05)
    plt.tight_layout()
    plt.show()

# Execution
# plot_null_model_comparison(G)

"""### III. Structural analysis"""

def analyze_top_residues_consensus(df_top_50):
    # 1. Count how many times each node appears in all columns (metrics)
    node_counts = df_top_50.stack().value_counts()

    # 2. Convert to DataFrame and calculate percentage
    n_metrics = len(df_top_50.columns)
    df_consensus = node_counts.reset_index()
    df_consensus.columns = ['Node', 'Frequency']
    df_consensus['Percentage'] = (df_consensus['Frequency'] / n_metrics) * 100

    # Extract information from node name (D/385/GLU)
    df_consensus['Residue'] = df_consensus['Node'].apply(lambda x: x.split('/')[-1])
    df_consensus['Number'] = df_consensus['Node'].apply(lambda x: x.split('/')[1])
    df_consensus['Chain'] = df_consensus['Node'].apply(lambda x: x.split('/')[0])

    # 3. Visualization: Persistence Histogram
    plt.figure(figsize=(10, 6))
    sns.histplot(df_consensus['Percentage'], bins=n_metrics, kde=False, color='teal')
    plt.title('Residue Persistence Across 7 Centrality Metrics')
    plt.xlabel('Presence Percentage (%)')
    plt.ylabel('Number of Residues')
    plt.grid(axis='y', alpha=0.3)
    plt.show()

    return df_consensus

# Execution:
# df_persistence = analyze_top_residues_consensus(df_top_50)

def plot_physicochemical_centrality_comparison(df_top_50):
    # 1. Define AA to Category mapping
    aa_categories = {
        'ALA':'Non-polar', 'GLY':'Non-polar', 'ILE':'Non-polar', 'LEU':'Non-polar',
        'MET':'Non-polar', 'PHE':'Non-polar', 'PRO':'Non-polar', 'TRP':'Non-polar', 'VAL':'Non-polar',
        'ARG':'Basic', 'HIS':'Basic', 'LYS':'Basic',
        'ASP':'Acidic', 'GLU':'Acidic',
        'SER':'Polar', 'THR':'Polar', 'ASN':'Polar', 'GLN':'Polar', 'CYS':'Polar', 'TYR':'Polar'
    }

    # 2. Process data to count categories for each metric
    summary_data = []

    for column in df_top_50.columns:
        # Extract AA code from each node in the Top 50 of that metric
        nodes = df_top_50[column].dropna()
        aa_codes = [n.split('/')[-1] for n in nodes]

        # Map to categories and count
        counts = pd.Series([aa_categories.get(aa, 'Unknown') for aa in aa_codes]).value_counts()

        # Save results
        for cat, count in counts.items():
            summary_data.append({'Metric': column, 'Category': cat, 'Count': count})

    df_summary = pd.DataFrame(summary_data)

    # 3. Create stacked bar chart
    # Pivot to have categories as columns
    df_pivot = df_summary.pivot(index='Metric', columns='Category', values='Count').fillna(0)

    # Define consistent colors for chemical properties
    colors = {
        'Non-polar': '#2ecc71', # Green (Hydrophobic)
        'Polar': '#3498db',     # Blue
        'Acidic': '#e74c3c',    # Red (Negative)
        'Basic': '#f1c40f',     # Yellow (Positive)
        'Unknown': '#95a5a6'
    }

    # Plot
    ax = df_pivot.plot(kind='bar', stacked=True, figsize=(12, 7),
                       color=[colors.get(x, '#95a5a6') for x in df_pivot.columns])

    plt.title('Chemical Profile of Top 50 Residues by Centrality Metric', fontsize=14)
    plt.ylabel('Number of Amino Acids')
    plt.xlabel('Centrality Metric')
    plt.legend(title='Chemical Property', bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

    return df_pivot

# Execution:
# df_chemical_count = plot_physicochemical_centrality_comparison(df_top_50)

def generate_complete_persistence_analysis(df_top_50):
    # 1. Consolidate all nodes that appear at least once in any Top 50
    # .stack() converts all columns into a single series
    all_nodes = df_top_50.stack().unique()

    # 2. Calculate persistence for each of these nodes
    n_metrics = len(df_top_50.columns)
    total_count = df_top_50.stack().value_counts()

    # 3. Create results DataFrame
    df_persistence = pd.DataFrame(index=all_nodes)
    df_persistence['Frequency'] = total_count
    df_persistence['Percentage'] = (df_persistence['Frequency'] / n_metrics) * 100

    # Reset index and clean up
    df_persistence = df_persistence.reset_index().rename(columns={'index': 'Node'})

    # Sort by percentage (highest to lowest)
    df_persistence = df_persistence.sort_values(by='Percentage', ascending=False).reset_index(drop=True)

    # 4. Generate Horizontal Histogram
    # Adjust figure size based on number of nodes (can be 100+)
    plt.figure(figsize=(10, len(df_persistence) * 0.25))

    # Define color palette that fades according to percentage
    sns.barplot(
        data=df_persistence,
        x='Percentage',
        y='Node',
        palette='viridis',
        edgecolor='black'
    )

    # Add guide lines and formatting
    plt.axvline(x=50, color='red', linestyle='--', alpha=0.5, label='50% Persistence')
    plt.axvline(x=100, color='gold', linestyle='-', alpha=0.7, label='100% Consensus')

    plt.title('Centrality Consensus: Persistence of Residues in the Analyses', fontsize=15)
    plt.xlabel('Presence Percentage in the 7 Metrics (%)', fontsize=12)
    plt.ylabel('Residue (Chain/Number/AA)', fontsize=12)
    plt.xlim(0, 105)
    plt.legend()
    plt.grid(axis='x', linestyle=':', alpha=0.6)

    plt.tight_layout()
    plt.show()

    return df_persistence

# Execution:
# df_key_residues = generate_complete_persistence_analysis(df_top_50)

# ============================================
# ANALYSIS OF INTERACTION TYPES
# ============================================

def analizar_tipos_interaccion(df_aristas, G):
    """
    Analyze the distribution of interaction types
    """
    print("\n=== ANALYSIS OF INTERACTION TYPES ===")

    # Verificar quÃ© columna contiene la informaciÃ³n de interacciÃ³n
    columnas_interaccion = []
    for col in ['interaction', 'type', 'interaction_type']:
        if col in df_aristas.columns and df_aristas[col].notna().any():
            columnas_interaccion.append(col)

    if columnas_interaccion:
        col_interaccion = columnas_interaccion[0]
        distribucion = df_aristas[col_interaccion].value_counts()

        print(f"Types of interaction found (columna: {col_interaccion}):")
        print(distribucion)

        # GrÃ¡fico de tipos de interacciÃ³n
        plt.figure(figsize=(12, 6))

        # Si hay muchos tipos, mostrar solo los top 15
        if len(distribucion) > 15:
            top_15 = distribucion.head(15)
            top_15.plot(kind='bar', color='lightseagreen')
            plt.title('Top 15 Most Common Types of Interaction')
        else:
            distribucion.plot(kind='bar', color='lightseagreen')
            plt.title('Distribution of Interaction Types')

        plt.xlabel('Type of Interaction')
        plt.ylabel('Frequency')
        plt.xticks(rotation=45, ha='right')
        plt.grid(True, alpha=0.3, axis='y')
        plt.tight_layout()
        plt.savefig('tipos_interaccion.png', dpi=300, bbox_inches='tight')
        plt.show()

        return distribucion
    else:
        print("No information was found on edge interaction types")
        return None

"""**Calculating properties by residue**"""

from Bio.PDB import PDBParser, ShrakeRupley
import propka.lib
import propka.run

def calculate_complete_properties(pdb_path):
    # 1. BioPython setup for SASA
    parser = PDBParser(QUIET=True)
    structure = parser.get_structure('protein', pdb_path)

    sr = ShrakeRupley()
    sr.compute(structure, level="R") # Calculate SASA at residue level

    # 2. PROPKA setup for pKa (New API)
    # Run propka internally to obtain calculated values
    pka_results = {}
    try:
        # Load molecule and calculate pKa
        molecule = propka.lib.load_molecule(pdb_path)
        pka_obj = propka.lib.Propka(molecule)
        pka_obj.calculate()

        # Map results: (Chain, Number) -> pKa
        for res in pka_obj.conformations['ALL'].residues:
            key = (res.chain, res.resnum)
            pka_results[key] = res.pka_value
    except Exception as e:
        print(f"âš ï¸ Note: PROPKA could not process some elements: {e}")

    # 3. Kyte-Doolittle scale
    kd_scale = {
        'ILE': 4.5, 'VAL': 4.2, 'LEU': 3.8, 'PHE': 2.8, 'CYS': 2.5,
        'MET': 1.9, 'ALA': 1.8, 'GLY': -0.4, 'THR': -0.7, 'SER': -0.8,
        'TRP': -0.9, 'TYR': -1.3, 'PRO': -1.6, 'HIS': -3.2, 'GLU': -3.5,
        'GLN': -3.5, 'ASP': -3.5, 'ASN': -3.5, 'LYS': -3.9, 'ARG': -4.5
    }

    # 4. Structure processing
    data = []

    for model in structure:
        for chain in model:
            for residue in chain:
                res_id = residue.get_id()
                res_name = residue.get_resname().strip()
                res_num = res_id[1]
                chain_id = chain.id

                # Classification by residue type
                het_type = res_id[0]
                if het_type == ' ':
                    res_type = 'Amino Acid'
                elif het_type == 'W':
                    res_type = 'Water'
                else:
                    # Ions are typically short names (1-2 letters), Ligands are longer
                    res_type = 'Ion' if len(res_name) <= 2 else 'Ligand'

                # Create ID: chain/number/name
                full_id = f"{chain_id}/{res_num}/{res_name}"

                # Extract properties
                sasa_val = getattr(residue, 'sasa', 0.0)
                pka_val = pka_results.get((chain_id, res_num), np.nan)
                hydro_val = kd_scale.get(res_name, np.nan)

                data.append({
                    'id': full_id,
                    'Type': res_type,
                    'SASA': round(sasa_val, 2),
                    'Adjusted_pKa': round(pka_val, 2) if not np.isnan(pka_val) else "N/A",
                    'Hydrophobicity': hydro_val
                })

    return pd.DataFrame(data)

# --- How to run ---
# df_bio = calculate_complete_properties("your_file.pdb")
# print(df_bio.to_string())

from Bio.PDB import PDBParser, ShrakeRupley, NeighborSearch

def analyze_allosteric_properties(pdb_path):
    parser = PDBParser(QUIET=True)
    structure = parser.get_structure('mol', pdb_path)

    # 1. Calculate SASA
    sr = ShrakeRupley()
    sr.compute(structure, level="R")

    # 2. Prepare neighbor search (for contact density/H-bonds)
    atoms = [a for a in structure.get_atoms()]
    ns = NeighborSearch(atoms)

    # 3. Kyte-Doolittle scale
    kd_scale = {
        'ILE': 4.5, 'VAL': 4.2, 'LEU': 3.8, 'PHE': 2.8, 'CYS': 2.5,
        'MET': 1.9, 'ALA': 1.8, 'GLY': -0.4, 'THR': -0.7, 'SER': -0.8,
        'TRP': -0.9, 'TYR': -1.3, 'PRO': -1.6, 'HIS': -3.2, 'GLU': -3.5,
        'GLN': -3.5, 'ASP': -3.5, 'ASN': -3.5, 'LYS': -3.9, 'ARG': -4.5
    }

    data = []
    for model in structure:
        for chain in model:
            for residue in chain:
                res_name = residue.get_resname().strip()
                res_num = residue.id[1]

                # Skip water molecules for this analysis
                if residue.id[0].startswith('W'): continue

                # A. Average B-Factor of residue (Mobility)
                b_factors = [atom.get_bfactor() for atom in residue]
                avg_b = np.mean(b_factors) if b_factors else 0

                # B. SASA
                sasa_val = getattr(residue, 'sasa', 0)

                # C. Hydrophobicity
                hydro = kd_scale.get(res_name, np.nan)

                # D. Local Contact Density (5.0 Ã… sphere)
                # Helps understand packing
                ca = residue['CA'] if 'CA' in residue else list(residue.get_atoms())[0]
                neighbors = ns.search(ca.coord, 5.0, level='R')
                contact_density = len(neighbors)

                data.append({
                    'id': f"{chain.id}/{res_num}/{res_name}",
                    'SASA': round(sasa_val, 2),
                    'B-Factor': round(avg_b, 2),
                    'Hydrophobicity': hydro,
                    'Packing_Density': contact_density
                })

    return pd.DataFrame(data)

# df_allostery = analyze_allosteric_properties("6EDW-R.pdb")

"""**Analyze betweeness versus B-factor**"""

from scipy.stats import spearmanr

def plot_network_structure_correlation(df_network, df_physical):
    """
    Merges centrality and structural properties data to analyze
    the relationship between communication and flexibility.
    """
    # 1. Merge DataFrames based on node ID
    # df_network['Node'] <-> df_physical['id']
    df_merged = pd.merge(
        df_network[['Node', 'Betweenness_Val']],
        df_physical[['id', 'B-Factor']],
        left_on='Node',
        right_on='id'
    ).drop('id', axis=1)

    # 2. Calculate Spearman correlation
    # (We use Spearman because Betweenness doesn't typically follow a normal distribution)
    coef, p_val = spearmanr(df_merged['Betweenness_Val'], df_merged['B-Factor'])

    # 3. Create the plot
    plt.figure(figsize=(10, 7))
    sns.set_style("whitegrid")

    # Scatter plot with regression line
    plot = sns.regplot(
        data=df_merged,
        x='B-Factor',
        y='Betweenness_Val',
        scatter_kws={'alpha':0.5, 'color': 'teal'},
        line_kws={'color': 'darkred', 'label': f'Spearman Ï: {coef:.2f}'}
    )

    plt.title('Correlation: Centrality (Betweenness) vs Flexibility (B-Factor)', fontsize=14)
    plt.xlabel('B-Factor (Flexibility/Mobility)', fontsize=12)
    plt.ylabel('Betweenness Centrality (Communication)', fontsize=12)
    plt.legend()

    # 4. Identify extreme cases (Interesting outliers)
    # Rigid residues (low B-factor) but critical for communication (high Betweenness)
    top_critical = df_merged[
        (df_merged['Betweenness_Val'] > df_merged['Betweenness_Val'].mean()) &
        (df_merged['B-Factor'] < df_merged['B-Factor'].mean())
    ].sort_values(by='Betweenness_Val', ascending=False).head(5)

    print(f"ðŸ“Š Spearman Correlation: {coef:.3f} (p-value: {p_val:.3e})")
    print("\nðŸš€ Candidates for Rigid Allosteric Centers (High Betweenness / Low B-Factor):")
    print(top_critical[['Node', 'Betweenness_Val', 'B-Factor']])

    plt.tight_layout()
    plt.show()

    return df_merged

# Execution:
# df_unified = plot_network_structure_correlation(df_full_G_S03, df_Structural_properties_G_S03)

"""**Code for calculating residues close to a specific residue indicated**"""

from Bio import PDB

def find_nearby_amino_acids(pdb_file, target_residue="MG", residue_number=804, distance=4.0):
    """
    Finds amino acids close to a specific residue
    """
    parser = PDB.PDBParser(QUIET=True)
    structure = parser.get_structure('structure', pdb_file)

    nearby_residues = []

    for model in structure:
        for chain in model:
            # Find target residue (MG 804)
            target_residue = None
            for residue in chain:
                if residue.id[0] == 'H_MG' or (residue.id[0] == ' ' and residue.resname == 'MG'):
                    if residue.id[1] == residue_number:
                        target_residue = residue
                        break

            if target_residue:
                # Get MG coordinates
                mg_atoms = [atom for atom in target_residue]
                if not mg_atoms:
                    continue

                # Calculate MG centroid
                mg_coords = np.array([atom.coord for atom in mg_atoms])
                mg_center = mg_coords.mean(axis=0)

                # Find nearby residues
                for residue in chain:
                    # Skip the MG itself and other non-amino acids
                    if residue.id[0] != ' ' or residue.resname == 'MG':
                        continue

                    # Calculate minimum distance between any residue atom and MG
                    min_distance = float('inf')
                    for atom in residue:
                        atom_distance = np.linalg.norm(atom.coord - mg_center)
                        min_distance = min(min_distance, atom_distance)

                    if min_distance <= distance:
                        nearby_residues.append({
                            'chain': chain.id,
                            'residue': residue.resname,
                            'number': residue.id[1],
                            'distance': min_distance
                        })

    return nearby_residues

# Example usage
# pdb_file = "structure.pdb"
# nearby_residues = find_nearby_amino_acids(pdb_file, target_residue="MG", residue_number=804, distance=4.0)

"""Percolation_sites"""

import random
from collections import defaultdict

def analyze_percolation(protein_graph, interaction_strengths=None):
    """
    Complete percolation analysis for protein stability.

    Args:
        protein_graph: NetworkX graph
        interaction_strengths: Dictionary of {(u, v): strength} for weighted analysis

    Returns:
        Dictionary with all percolation metrics
    """
    results = {}

    # 1. Random site percolation
    df_random = site_percolation(protein_graph, method='random')

    # 2. Targeted site percolation (attack on hubs)
    df_targeted = site_percolation(protein_graph, method='by_degree')

    # 3. Find percolation threshold (p_c)
    # p_c is where relative size drops below 0.5
    p_c_random = find_percolation_threshold(df_random)
    p_c_targeted = find_percolation_threshold(df_targeted)

    # 4. Link percolation if interaction strengths provided
    if interaction_strengths is not None:
        df_link = link_percolation(protein_graph, interaction_strengths)
        p_c_link = find_percolation_threshold(df_link)
        results['link_percolation'] = df_link
        results['p_c_link'] = p_c_link

    # 5. Robustness metric (area under curve)
    robustness_random = calculate_robustness(df_random)
    robustness_targeted = calculate_robustness(df_targeted)

    # 6. Visualize
    visualize_percolation_results(df_random, df_targeted)

    # Store results
    results.update({
        'random_percolation': df_random,
        'targeted_percolation': df_targeted,
        'p_c_random': p_c_random,
        'p_c_targeted': p_c_targeted,
        'robustness_random': robustness_random,
        'robustness_targeted': robustness_targeted,
        'fragility_index': p_c_targeted - p_c_random  # Negative = fragile to targeted attacks
    })

    return results

def find_percolation_threshold(df):
    """
    Find percolation threshold where relative size drops below 0.5.
    """
    if df.empty:
        return None

    # Find where relative_size crosses 0.5
    below_half = df[df['relative_size'] < 0.5]
    if not below_half.empty:
        return below_half.iloc[0]['fraction_eliminated']
    else:
        return 1.0  # Never drops below 0.5

def calculate_robustness(df):
    """
    Calculate robustness as area under the percolation curve.
    """
    if df.empty:
        return 0

    # Sort by fraction_eliminated
    df_sorted = df.sort_values('fraction_eliminated')

    # Use trapezoidal rule for area
    x = df_sorted['fraction_eliminated'].values
    y = df_sorted['relative_size'].values

    # Add point at (0, 1) if not present
    if x[0] > 0:
        x = np.concatenate([[0], x])
        y = np.concatenate([[1], y])

    # Add point at (1, 0) if not present
    if x[-1] < 1:
        x = np.concatenate([x, [1]])
        y = np.concatenate([y, [0]])

    # Calculate area (robustness)
    robustness = np.trapz(y, x)

    return robustness

def link_percolation(protein_graph, interaction_strengths):
    """
    Link percolation by eliminating interactions based on strength.
    """
    results = []
    edges = list(protein_graph.edges(data=True))

    # Sort edges from weakest to strongest
    edges_sorted = sorted(
        edges,
        key=lambda x: interaction_strengths.get((x[0], x[1]), 0)
    )

    G_temp = protein_graph.copy()

    # Create thresholds from weakest to strongest
    strengths = [interaction_strengths.get((u, v), 0) for u, v, _ in edges_sorted]
    if strengths:
        thresholds = np.percentile(strengths, np.arange(0, 101, 5))
    else:
        thresholds = np.arange(0, 101, 5)

    for i, (fraction_eliminated, threshold) in enumerate(zip(
        np.arange(0, 1.01, 0.05),
        thresholds
    )):
        # Remove edges with strength < threshold
        edges_to_remove = [
            (u, v) for u, v, d in G_temp.edges(data=True)
            if interaction_strengths.get((u, v), 0) < threshold
        ]
        G_temp.remove_edges_from(edges_to_remove)

        # Calculate connectivity
        if nx.is_connected(G_temp):
            relative_size = 1.0
        else:
            largest_component = max(nx.connected_components(G_temp), key=len)
            relative_size = len(largest_component) / protein_graph.number_of_nodes()

        results.append({
            'fraction_eliminated': fraction_eliminated,
            'relative_size': relative_size,
            'threshold': threshold,
            'num_components': nx.number_connected_components(G_temp)
        })

    return pd.DataFrame(results)

def visualize_percolation_results(df_random, df_targeted):
    """
    Visualize percolation curves.
    """
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))

    # Plot 1: Percolation curves
    axes[0].plot(df_random['fraction_eliminated'], df_random['relative_size'],
                'b-o', label='Random attack', linewidth=2, markersize=6)
    axes[0].plot(df_targeted['fraction_eliminated'], df_targeted['relative_size'],
                'r-s', label='Targeted attack (hubs)', linewidth=2, markersize=6)

    # Add threshold lines
    p_c_random = find_percolation_threshold(df_random)
    p_c_targeted = find_percolation_threshold(df_targeted)

    if p_c_random is not None:
        axes[0].axvline(p_c_random, color='blue', linestyle='--', alpha=0.5,
                       label=f'p_c random = {p_c_random:.2f}')
    if p_c_targeted is not None:
        axes[0].axvline(p_c_targeted, color='red', linestyle='--', alpha=0.5,
                       label=f'p_c targeted = {p_c_targeted:.2f}')

    axes[0].axhline(0.5, color='gray', linestyle=':', alpha=0.7)
    axes[0].set_xlabel('Fraction of residues eliminated')
    axes[0].set_ylabel('Relative size of largest component')
    axes[0].set_title('Site Percolation Curves')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    # Plot 2: Number of components
    axes[1].plot(df_random['fraction_eliminated'], df_random['num_components'],
                'b-o', label='Random', linewidth=2, markersize=6)
    axes[1].plot(df_targeted['fraction_eliminated'], df_targeted['num_components'],
                'r-s', label='Targeted', linewidth=2, markersize=6)
    axes[1].set_xlabel('Fraction of residues eliminated')
    axes[1].set_ylabel('Number of connected components')
    axes[1].set_title('Fragmentation during Percolation')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    # Print summary statistics
    print("=== PERCOLATION ANALYSIS SUMMARY ===")
    print(f"Percolation threshold (random): {p_c_random:.3f}")
    print(f"Percolation threshold (targeted): {p_c_targeted:.3f}")
    print(f"Robustness to random attacks: {calculate_robustness(df_random):.3f}")
    print(f"Robustness to targeted attacks: {calculate_robustness(df_targeted):.3f}")
    print(f"Fragility index (p_c_targeted - p_c_random): {p_c_targeted - p_c_random:.3f}")

    # Interpretation
    fragility = p_c_targeted - p_c_random
    if fragility < -0.2:
        print("\nINTERPRETATION: Protein is FRAGILE - highly sensitive to mutations in hubs")
    elif fragility < -0.05:
        print("\nINTERPRETATION: Protein is MODERATELY ROBUST")
    else:
        print("\nINTERPRETATION: Protein is ROBUST - resistant to both random and targeted attacks")

# FunciÃ³n site_percolation corregida (la misma de arriba)
def site_percolation(protein_graph, method='random'):
    """
    Site percolation by eliminating residues.
    """
    results = []
    nodes = list(protein_graph.nodes())

    for eliminated_fraction in np.arange(0, 0.96, 0.05):
        G_temp = protein_graph.copy()

        if method == 'random':
            to_eliminate = random.sample(nodes, int(eliminated_fraction * len(nodes)))
        elif method == 'by_degree':
            degrees = dict(G_temp.degree())
            to_eliminate = sorted(degrees.keys(), key=lambda x: degrees[x], reverse=True)
            to_eliminate = to_eliminate[:int(eliminated_fraction * len(nodes))]
        else:
            raise ValueError("Method must be 'random' or 'by_degree'")

        G_temp.remove_nodes_from(to_eliminate)

        if G_temp.number_of_nodes() == 0:
            relative_size = 0
            num_components = 0
        else:
            components = list(nx.connected_components(G_temp))
            max_size = max([len(c) for c in components]) if components else 0
            relative_size = max_size / protein_graph.number_of_nodes()
            num_components = len(components)

        results.append({
            'fraction_eliminated': eliminated_fraction,
            'relative_size': relative_size,
            'num_components': num_components,
            'method': method
        })

    return pd.DataFrame(results)

"""## Application

## ICL2 Apo form analysis

#### a) Uploading and network generation
"""

# Upload data GPCR-Drug complex
json_path = "6EDW-R-Network.json"
data01 = load_ring_json_data(json_path)

# Extract nodes and edges
nodes_S01, edges_S01 = extract_nodes_edges(data01)
# Create DataFrames
df_nodes_S01 = pd.DataFrame(nodes_S01)
df_edges_S01 = pd.DataFrame(edges_S01)

df_nodes_S01

df_edges_S01

# Save DataFrames
df_nodes_S01.to_csv('protein_nodes.csv', index=False)
df_edges_S01.to_csv('protein_interactions.csv', index=False)

# Create Network
G_S01 = create_graph_from_dataframes(df_nodes_S01, df_edges_S01)

"""**Basic descriptors**"""

print("\n=== GRAPH INFORMATION ===")
print(f"Number of nodes: {G_S01.number_of_nodes()}")
print(f"Number of edges: {G_S01.number_of_edges()}")
print(f"Graph density: {nx.density(G_S01):.4f}")
print(f"Is the graph connected? {nx.is_connected(G_S01)}")

# Additional information about nodes
if G_S01.number_of_nodes() > 0:
    first_node = list(G_S01.nodes(data=True))[0]
    print(f"\nNode example: {first_node[0]}")
    print(f"Node attributes: {first_node[1]}")

"""### Analisis 1

#### **a) Clustering: average clustering coefficient.**
"""

df_Clustering_coefficient_G_S01=analyze_clustering(G_S01)

"""#### **b) Distances: average shortestâ€“path length and (effective) diameter.**"""

dict_Distances_G_S01=analyze_distances(G_S01)

"""#### **c) Degree statistics:**

**Degree distribution**
"""

# Generar grÃ¡ficos de distribuciÃ³n de grado
degrees_S01 = plot_distribucion_grado(G_S01)

"""**Degree statistics: Network topology analysis**"""

analyze_degree_statistics(G_S01)

"""**Discussion**

- **$Î±=2.61$:** This value falls within the typical range for "small-world" networks (usually between 2<Î±<3) but the likelihood ratio tests indicate otherwise. Then, this number is only an estimate of the slope in a specific segment, but does not define the overall nature of the network.
- **Negative R (R<0):** In both the comparison with the Exponential (R=âˆ’342.53) and Log-Normal (R=âˆ’458.48) models, the negative sign indicates that the second model is much better than Power Law at explaining the data.
- **Statistical Significance (p=0.0000):** Because it is less than 0.05, with complete certainty these results are not due to chance.

After performing a goodness-of-fit analysis using the Likelihood Ratio Test, the hypothesis of a Power Law distribution was rejected (p<0.05). The data show a statistically significant preference for a Log-Normal distribution (R=âˆ’458.48), suggesting that the network does not have a scale-free architecture, but rather exhibits saturation in the connectivity of the most important nodes. In terms of protein physics, the coexistence of a Log-Normal degree distribution with Small-world properties reveals how evolution has optimized the protein structure to be efficient:
- **Log-Normal distribution**:  The fact that the likelihood ratio (R) test favors the Log-Normal distribution over the Power Law distribution is due to spatial and volume exclusion constraints (Steric Saturation). In a social network, a "hub" can have infinite connections but in a protein, a residue has a limited physical space around it. Fifty residues cannot fit in direct contact with a single residue. This creates a "saturation point" that cuts off the tail of the distribution (typical of a Log-Normal distribution).
- **Small-world propertie**: The value of Î±=2.61 is in the range typically associated with scale-free networks (2-3), but even if the network is not purely scale-free (as the R value proved), this exponent reflects a Small-World topology:
    * **Structural Shortcuts (Long-range contacts):** Protein folding brings together residues that are far apart in the primary sequence (in the linear chain). These long-range contacts act as "bridges" or shortcuts.
    * **High Connectivity of Critical Nodes:** Although there are not "super-hubs" of 500 edges, there are residues with above-average connectivity (residues in the hydrophobic core). A value of Î± of 2.61 indicates that there are enough nodes with moderately high connectivity to keep the entire protein connected with very few steps (an average of 3 to 5 jumps between any pair of residues). The protein needs to transmit allosteric signals (information) rapidly from one end to the other for its catalytic or binding function. Therefore, it maintains a low Î± value, which ensures that the shortest path between residues is short.

#### **d) Centrality structure:**

**Centrality_distribution**
"""

# Realizar anÃ¡lisis de centralidad
centralidad_df_S01 = analyze_centrality(G_S01)

"""**Centrality structure: Rank the top 50 nodes**"""

df_full_G_S01, df_top50_G_S01 = analyze_centrality_rankings(G_S01)

# Guardar DataFrame como CSV
df_full_G_S01.to_csv('G_S01_full_centrality.csv', index=False)

# Open CSV as DataFrame
df_full_G_S01 = pd.read_csv('G_S01_full_centrality.csv')

df_full_G_S01

# Guardar DataFrame como CSV
df_top50_G_S01.to_csv('G_S01_top50_centrality.csv', index=False)

# Open CSV as DataFrame
df_top50_G_S01 = pd.read_csv('G_S01_top50_centrality.csv')

df_top50_G_S01

df_persistencedf_top50_G_S01 = analyze_top_residues_consensus(df_top50_G_S01)

df_chemical_count_G_S01 = plot_physicochemical_centrality_comparison(df_top50_G_S01)

df_key_residues_G_S01 = generate_complete_persistence_analysis(df_top50_G_S01)

#visualize_persistence_3d(df_key_residues_G_S01, pdb_path='./6EDW-R.pdb')
visualize_persistence_3d_clean(df_key_residues_G_S01, pdb_path='./6EDW-R.pdb')

visualize_3d_persistence_pro(df_key_residues_G_S01, pdb_path='./6EDW-R.pdb', show_labels=False)

nearby_residues_MG804 = find_nearby_amino_acids('./6EDW-R.pdb', target_residue="MG", residue_number=804, distance=4.0)
df_nearby_MG804 = pd.DataFrame(nearby_residues)
df_nearby_MG804

"""#### d) Null models

Null models: generate ten ErdÅ‘sâ€“RÃ©nyi (ER) and ten BarabÃ¡siâ€“Albert (BA) networks with the same number of nodes and edges as your real network.
"""

df_nulls_G_S01 = null_models_analysis(G_S01)
df_nulls_G_S01

plot_null_model_comparison(G_S01)

"""#### f) Community analysis

**Infomap Community analysis**
"""

result_S01 = analyze_and_visualize_infomap(G_S01)

"""**Comparative Community analysis**"""

df_resultados_Community_analysis_G_S01 = comparative_community_analysis(G_S01, "G_S01_mi_analisis_completo.csv")

# Reading the generated Community Data
nombre_archivo_G_S01 = "G_S01_mi_analisis_completo.csv"
df_community_analisis_G_S01 = pd.read_csv(nombre_archivo_G_S01)
df_community_analisis_G_S01

# Count community sizes by method and graph them
plt.figure(figsize=(10, 5))
for metodo in metodos:
    counts_G_S01 = df_community_analisis_G_S01[metodo].value_counts().values
    sns.kdeplot(counts_G_S01, label=metodo, fill=True, alpha=0.3)

plt.xlabel("Community Size (Number of Nodes)")
plt.ylabel("Density")
plt.title("Community Size Distribution")
plt.legend()
plt.show()

"""**Consensus analysis between community analysis algorithms**"""

from sklearn.metrics import adjusted_rand_score
import seaborn as sns

# Calculate similarity matrix between columns
metodos = df_community_analisis_G_S01.columns[1:] # Excluimos la columna 'Nodo'
ari_matrix_G_S01 = pd.DataFrame(index=metodos, columns=metodos)

for m1 in metodos:
    for m2 in metodos:
        ari_matrix_G_S01.loc[m1, m2] = adjusted_rand_score(df_community_analisis_G_S01[m1], df_community_analisis_G_S01[m2])

plt.figure(figsize=(8, 6))
sns.heatmap(ari_matrix_G_S01.astype(float), annot=True, cmap='Blues')
plt.title("Similarity betwenn Algoritmos (ARI) in apo form ICL2 structure",
          pad=20)
plt.show()

visualize_local_pdb_communities(df_community_analisis_G_S01, 'Infomap', './6EDW-R.pdb', n_communities=4)

visualize_local_pdb_communities(df_community_analisis_G_S01, 'Girvan_Newman', './6EDW-R.pdb', n_communities=4)



"""**Which nodes are most related?**"""

nodos = df_community_analisis_G_S01['Node'].values
n = len(nodos)
co_occurrence = np.zeros((n, n))

# Iterate through each pair of nodes
for i in range(n):
    for j in range(i + 1, n):
        # Count how many columns have the same values â€‹â€‹in row i and j
        matches = (df_community_analisis_G_S01.iloc[i, 1:] == df_community_analisis_G_S01.iloc[j, 1:]).sum()
        co_occurrence[i, j] = co_occurrence[j, i] = matches

# Visualize a sample or the nodes with the highest relationship
plt.figure(figsize=(10, 8))
sns.heatmap(co_occurrence[:30, :30], xticklabels=nodos[:30], yticklabels=nodos[:30], cmap="YlGnBu")
plt.title("Consensus Matrix (First 30 nodes)")
plt.show()

# Consensus Matrix
matriz_total_G_S01 = generate_consensus_matrix(df_community_analisis_G_S01)

# VisualizaciÃ³n (si n_nodos es manejable, ej. < 200)
plt.figure(figsize=(24, 20))
sns.heatmap(matriz_total_G_S01, cmap="YlGnBu", xticklabels=False, yticklabels=False)
plt.title("Complete Consensus Matrix (All nodes)")
plt.savefig("G_S01_Consensus Matrix.png", dpi=300, bbox_inches='tight')
plt.savefig("G_S01_Consensus Matrix.pdf", bbox_inches='tight')
plt.savefig("G_S01_Consensus Matrix.svg", bbox_inches='tight')
plt.show()

"""**Generate Consensus graph**"""

df_gold_relationships_G_S01, network_gold_relationships_G_S01 = extract_consensus_and_plot(df_community_analisis_G_S01, threshold=0.7)

"""**Umbral analysis in concensus graph**"""

df_gold_relationships_G_S01

df_multi_threshold_G_S01 = generate_multi_threshold_consensus_csv(df_community_analisis_G_S01, thresholds=[1.0, 0.9, 0.8, 0.7, 0.6])

df_multi_threshold_G_S01

plot_unit_count(df_multi_threshold_G_S01)

#Execute Sankey passing that specific dataframe
generate_filtered_sankey(df_multi_threshold_G_S01, min_nodes=5)

# (Opcional) Establecer el nodo como el Ã­ndice para realizar bÃºsquedas rÃ¡pidas
# df_analisis.set_index('Nodo', inplace=True)

# 3. Mostrar las primeras filas y un resumen rÃ¡pido
print("Vista previa del anÃ¡lisis comparativo:")
print(df_community_analisis_G_S01.head())

print("\nConteo de comunidades por cada mÃ©todo:")
print(df_community_analisis_G_S01.nunique())

""" #### e) Structural analysis"""

# Analizar tipos de interacciÃ³n
distribucion_interacciones_S01 = analizar_tipos_interaccion(df_aristas_S01, G_S01)



"""## ICL2 bound Acetyl-Coa form analysis I

#### a) Uploading and network generation
"""

# Upload data GPCR-Drug complex
json_path = "6EDZ-R-Network.json"
data02 = load_ring_json_data(json_path)

# Extract nodes and edges
nodes_S02, edges_S02 = extract_nodes_edges(data02)
# Create DataFrames
df_nodes_S02 = pd.DataFrame(nodes_S02)
df_edges_S02 = pd.DataFrame(edges_S02)

df_nodes_S02

df_edges_S02

# Save DataFrames
df_nodes_S02.to_csv('6EDZ-protein_nodes.csv', index=False)
df_edges_S02.to_csv('6EDZ-protein_interactions.csv', index=False)

# Create Network
G_S02 = create_graph_from_dataframes(df_nodes_S02, df_edges_S02)

"""**Basic descriptors**"""

print("\n=== GRAPH INFORMATION ===")
print(f"Number of nodes: {G_S02.number_of_nodes()}")
print(f"Number of edges: {G_S02.number_of_edges()}")
print(f"Graph density: {nx.density(G_S02):.4f}")
print(f"Is the graph connected? {nx.is_connected(G_S02)}")

# Additional information about nodes
if G_S02.number_of_nodes() > 0:
    first_node = list(G_S02.nodes(data=True))[0]
    print(f"\nNode example: {first_node[0]}")
    print(f"Node attributes: {first_node[1]}")

"""### Analisis

#### **a) Clustering: average clustering coefficient.**
"""

df_Clustering_coefficient_G_S02=analyze_clustering(G_S02)

"""#### **b) Distances: average shortestâ€“path length and (effective) diameter.**"""

dict_Distances_G_S02=analyze_distances(G_S02)

"""#### **c) Degree statistics:**

**Degree distribution**
"""

# Generar grÃ¡ficos de distribuciÃ³n de grado
degrees_S02 = plot_distribucion_grado(G_S02)

"""**Degree statistics: Network topology analysis**"""

analyze_degree_statistics(G_S02)

"""**Discussion**

- **$Î±=2.61$:** This value falls within the typical range for "small-world" networks (usually between 2<Î±<3) but the likelihood ratio tests indicate otherwise. Then, this number is only an estimate of the slope in a specific segment, but does not define the overall nature of the network.
- **Negative R (R<0):** In both the comparison with the Exponential (R=âˆ’342.53) and Log-Normal (R=âˆ’458.48) models, the negative sign indicates that the second model is much better than Power Law at explaining the data.
- **Statistical Significance (p=0.0000):** Because it is less than 0.05, with complete certainty these results are not due to chance.

After performing a goodness-of-fit analysis using the Likelihood Ratio Test, the hypothesis of a Power Law distribution was rejected (p<0.05). The data show a statistically significant preference for a Log-Normal distribution (R=âˆ’458.48), suggesting that the network does not have a scale-free architecture, but rather exhibits saturation in the connectivity of the most important nodes. In terms of protein physics, the coexistence of a Log-Normal degree distribution with Small-world properties reveals how evolution has optimized the protein structure to be efficient:
- **Log-Normal distribution**:  The fact that the likelihood ratio (R) test favors the Log-Normal distribution over the Power Law distribution is due to spatial and volume exclusion constraints (Steric Saturation). In a social network, a "hub" can have infinite connections but in a protein, a residue has a limited physical space around it. Fifty residues cannot fit in direct contact with a single residue. This creates a "saturation point" that cuts off the tail of the distribution (typical of a Log-Normal distribution).
- **Small-world propertie**: The value of Î±=2.61 is in the range typically associated with scale-free networks (2-3), but even if the network is not purely scale-free (as the R value proved), this exponent reflects a Small-World topology:
    * **Structural Shortcuts (Long-range contacts):** Protein folding brings together residues that are far apart in the primary sequence (in the linear chain). These long-range contacts act as "bridges" or shortcuts.
    * **High Connectivity of Critical Nodes:** Although there are not "super-hubs" of 500 edges, there are residues with above-average connectivity (residues in the hydrophobic core). A value of Î± of 2.61 indicates that there are enough nodes with moderately high connectivity to keep the entire protein connected with very few steps (an average of 3 to 5 jumps between any pair of residues). The protein needs to transmit allosteric signals (information) rapidly from one end to the other for its catalytic or binding function. Therefore, it maintains a low Î± value, which ensures that the shortest path between residues is short.

#### **d) Centrality structure:**

**Centrality_distribution**
"""

# Realizar anÃ¡lisis de centralidad
centralidad_df_S02 = analyze_centrality(G_S02)

"""**Centrality structure: Rank the top 50 nodes**"""

df_full_G_S02, df_top50_G_S02 = analyze_centrality_rankings(G_S02)

# Guardar DataFrame como CSV
df_full_G_S02.to_csv('G_S02_full_centrality.csv', index=False)

# Open CSV as DataFrame
df_full_G_S02 = pd.read_csv('G_S02_full_centrality.csv')

df_full_G_S02

# Guardar DataFrame como CSV
df_top50_G_S02.to_csv('G_S02_top50_centrality.csv', index=False)

# Open CSV as DataFrame
df_top50_G_S02 = pd.read_csv('G_S02_top50_centrality.csv')

df_top50_G_S02

df_persistencedf_top50_G_S02 = analyze_top_residues_consensus(df_top50_G_S02)

df_chemical_count_G_S02 = plot_physicochemical_centrality_comparison(df_top50_G_S02)

df_key_residues_G_S02 = generate_complete_persistence_analysis(df_top50_G_S02)

#visualize_persistence_3d_clean(df_key_residues_G_S01, pdb_path='./6EDW-R.pdb')
visualize_persistence_3d_clean(df_key_residues_G_S02, pdb_path='./6EDZ-R.pdb')

visualize_3d_persistence_pro(df_key_residues_G_S02, pdb_path='./6EDZ-R.pdb', show_labels=False)

nearby_residues_ACO801 = find_nearby_amino_acids('./6EDZ-R.pdb', target_residue="ACO", residue_number=801, distance=4.0)
df_nearby_ACO801 = pd.DataFrame(nearby_residues_ACO801)
df_nearby_ACO801

"""#### d) Null models

Null models: generate ten ErdÅ‘sâ€“RÃ©nyi (ER) and ten BarabÃ¡siâ€“Albert (BA) networks with the same number of nodes and edges as your real network.
"""

df_nulls_G_S02 = null_models_analysis(G_S02)
df_nulls_G_S02

plot_null_model_comparison(G_S02)

"""#### f) Community analysis

**Infomap Community analysis**
"""

result_S02 = analyze_and_visualize_infomap(G_S02)

"""**Comparative Community analysis**"""

df_resultados_Community_analysis_G_S02 = comparative_community_analysis(G_S02, "G_S02_mi_analisis_completo.csv")

# Reading the generated Community Data
nombre_archivo_G_S02 = "G_S02_mi_analisis_completo.csv"
df_community_analisis_G_S02 = pd.read_csv(nombre_archivo_G_S02)
df_community_analisis_G_S02

# Definir los mÃ©todos que estÃ¡s analizando
metodos = ['Infomap', 'Modularity', 'Label_Propagation', 'Girvan_Newman', 'K_Clique_3']

# Count community sizes by method and graph them
plt.figure(figsize=(10, 5))
for metodo in metodos:
    counts_G_S02 = df_community_analisis_G_S02[metodo].value_counts().values
    sns.kdeplot(counts_G_S02, label=metodo, fill=True, alpha=0.3)

plt.xlabel("Community Size (Number of Nodes)")
plt.ylabel("Density")
plt.title("Community Size Distribution")
plt.legend()
plt.show()

"""**Consensus analysis between community analysis algorithms**"""

from sklearn.metrics import adjusted_rand_score
import seaborn as sns

# Calculate similarity matrix between columns
metodos = df_community_analisis_G_S02.columns[1:] # Excluimos la columna 'Nodo'
ari_matrix_G_S02 = pd.DataFrame(index=metodos, columns=metodos)

for m1 in metodos:
    for m2 in metodos:
        ari_matrix_G_S02.loc[m1, m2] = adjusted_rand_score(df_community_analisis_G_S02[m1], df_community_analisis_G_S02[m2])

plt.figure(figsize=(8, 6))
sns.heatmap(ari_matrix_G_S02.astype(float), annot=True, cmap='Blues')
plt.title("Similarity betwenn Algoritmos (ARI) in apo form ICL2 structure",
          pad=20)
plt.show()

visualize_local_pdb_communities(df_community_analisis_G_S02, 'Infomap', './6EDZ-R.pdb', n_communities=4)

visualize_local_pdb_communities(df_community_analisis_G_S02, 'Girvan_Newman', './6EDZ-R.pdb', n_communities=4)



"""**Which nodes are most related?**"""

nodos = df_community_analisis_G_S02['Node'].values
n = len(nodos)
co_occurrence = np.zeros((n, n))

# Iterate through each pair of nodes
for i in range(n):
    for j in range(i + 1, n):
        # Count how many columns have the same values â€‹â€‹in row i and j
        matches = (df_community_analisis_G_S02.iloc[i, 1:] == df_community_analisis_G_S02.iloc[j, 1:]).sum()
        co_occurrence[i, j] = co_occurrence[j, i] = matches

# Visualize a sample or the nodes with the highest relationship
plt.figure(figsize=(10, 8))
sns.heatmap(co_occurrence[:30, :30], xticklabels=nodos[:30], yticklabels=nodos[:30], cmap="YlGnBu")
plt.title("Consensus Matrix (First 30 nodes)")
plt.show()

# Consensus Matrix
matriz_total_G_S02 = generate_consensus_matrix(df_community_analisis_G_S02)

# VisualizaciÃ³n (si n_nodos es manejable, ej. < 200)
plt.figure(figsize=(24, 20))
sns.heatmap(matriz_total_G_S02, cmap="YlGnBu", xticklabels=False, yticklabels=False)
plt.title("Complete Consensus Matrix (All nodes)")
plt.savefig("G_S02_Consensus Matrix.png", dpi=300, bbox_inches='tight')
plt.savefig("G_S02_Consensus Matrix.pdf", bbox_inches='tight')
plt.savefig("G_S02_Consensus Matrix.svg", bbox_inches='tight')
plt.show()

"""**Generate Consensus graph**"""

df_gold_relationships_G_S02, network_gold_relationships_G_S02 = extract_consensus_and_plot(df_community_analisis_G_S02, threshold=0.7)

"""**Umbral analysis in concensus graph**"""

df_gold_relationships_G_S02

df_multi_threshold_G_S02 = generate_multi_threshold_consensus_csv(df_community_analisis_G_S02, thresholds=[1.0, 0.9, 0.8, 0.7, 0.6])

df_multi_threshold_G_S02

plot_unit_count(df_multi_threshold_G_S02)

#Execute Sankey passing that specific dataframe
generate_filtered_sankey(df_multi_threshold_G_S02, min_nodes=5)

# (Opcional) Establecer el nodo como el Ã­ndice para realizar bÃºsquedas rÃ¡pidas
# df_analisis.set_index('Nodo', inplace=True)

# 3. Mostrar las primeras filas y un resumen rÃ¡pido
print("Vista previa del anÃ¡lisis comparativo:")
print(df_community_analisis_G_S02.head())

print("\nConteo de comunidades por cada mÃ©todo:")
print(df_community_analisis_G_S02.nunique())

""" #### e) Structural analysis"""

# Analizar tipos de interacciÃ³n
distribucion_interacciones_S02 = analizar_tipos_interaccion(df_edges_S02, G_S02)



"""## ICL2 bound Acetyl-Coa form analysis II

#### a) Uploading and network generation
"""

# Upload data GPCR-Drug complex
json_path = "6EE1-R-Network.json"
data03 = load_ring_json_data(json_path)

# Extract nodes and edges
nodes_S03, edges_S03 = extract_nodes_edges(data03)
# Create DataFrames
df_nodes_S03 = pd.DataFrame(nodes_S03)
df_edges_S03 = pd.DataFrame(edges_S03)

df_nodes_S03

df_edges_S03

# Save DataFrames
df_nodes_S03.to_csv('6EE1_protein_nodes.csv', index=False)
df_edges_S03.to_csv('6EE1_protein_interactions.csv', index=False)

# Create Network
G_S03 = create_graph_from_dataframes(df_nodes_S03, df_edges_S03)

G_S03

"""**Basic descriptors**"""

print("\n=== GRAPH INFORMATION ===")
print(f"Number of nodes: {G_S03.number_of_nodes()}")
print(f"Number of edges: {G_S03.number_of_edges()}")
print(f"Graph density: {nx.density(G_S03):.4f}")
print(f"Is the graph connected? {nx.is_connected(G_S03)}")

# Additional information about nodes
if G_S03.number_of_nodes() > 0:
    first_node = list(G_S03.nodes(data=True))[0]
    print(f"\nNode example: {first_node[0]}")
    print(f"Node attributes: {first_node[1]}")

"""### Analisis

#### **a) Clustering: average clustering coefficient.**
"""

df_Clustering_coefficient_G_S03=analyze_clustering(G_S03)

"""#### **b) Distances: average shortestâ€“path length and (effective) diameter.**"""

dict_Distances_G_S03=analyze_distances(G_S03)

"""#### **c) Degree statistics:**

**Degree distribution**
"""

# Generar grÃ¡ficos de distribuciÃ³n de grado
degrees_S03 = plot_distribucion_grado(G_S03)

"""**Degree statistics: Network topology analysis**"""

analyze_degree_statistics(G_S03)

"""**Discussion**

- **$Î±=2.61$:** This value falls within the typical range for "small-world" networks (usually between 2<Î±<3) but the likelihood ratio tests indicate otherwise. Then, this number is only an estimate of the slope in a specific segment, but does not define the overall nature of the network.
- **Negative R (R<0):** In both the comparison with the Exponential (R=âˆ’342.53) and Log-Normal (R=âˆ’458.48) models, the negative sign indicates that the second model is much better than Power Law at explaining the data.
- **Statistical Significance (p=0.0000):** Because it is less than 0.05, with complete certainty these results are not due to chance.

After performing a goodness-of-fit analysis using the Likelihood Ratio Test, the hypothesis of a Power Law distribution was rejected (p<0.05). The data show a statistically significant preference for a Log-Normal distribution (R=âˆ’458.48), suggesting that the network does not have a scale-free architecture, but rather exhibits saturation in the connectivity of the most important nodes. In terms of protein physics, the coexistence of a Log-Normal degree distribution with Small-world properties reveals how evolution has optimized the protein structure to be efficient:
- **Log-Normal distribution**:  The fact that the likelihood ratio (R) test favors the Log-Normal distribution over the Power Law distribution is due to spatial and volume exclusion constraints (Steric Saturation). In a social network, a "hub" can have infinite connections but in a protein, a residue has a limited physical space around it. Fifty residues cannot fit in direct contact with a single residue. This creates a "saturation point" that cuts off the tail of the distribution (typical of a Log-Normal distribution).
- **Small-world propertie**: The value of Î±=2.61 is in the range typically associated with scale-free networks (2-3), but even if the network is not purely scale-free (as the R value proved), this exponent reflects a Small-World topology:
    * **Structural Shortcuts (Long-range contacts):** Protein folding brings together residues that are far apart in the primary sequence (in the linear chain). These long-range contacts act as "bridges" or shortcuts.
    * **High Connectivity of Critical Nodes:** Although there are not "super-hubs" of 500 edges, there are residues with above-average connectivity (residues in the hydrophobic core). A value of Î± of 2.61 indicates that there are enough nodes with moderately high connectivity to keep the entire protein connected with very few steps (an average of 3 to 5 jumps between any pair of residues). The protein needs to transmit allosteric signals (information) rapidly from one end to the other for its catalytic or binding function. Therefore, it maintains a low Î± value, which ensures that the shortest path between residues is short.

#### **d) Centrality structure:**

**Centrality_distribution**
"""

# Realizar anÃ¡lisis de centralidad
centralidad_df_S03 = analyze_centrality(G_S03)

"""**Centrality structure: Rank the top 50 nodes**"""

def analyze_centrality_rankings(G, top_n=50):
    print("Calculating centrality metrics... this may take a moment.")

    # 1. Centrality Calculations
    dict_centralities = {
        'Degree': nx.degree_centrality(G),
        'Closeness': nx.closeness_centrality(G),
        'Betweenness': nx.betweenness_centrality(G),
        'Eigenvector': nx.eigenvector_centrality(G, max_iter=1000),
        'Katz': nx.katz_centrality(G, alpha=0.1, beta=1.0),
        'PageRank': nx.pagerank(G),
        'Subgraph': nx.subgraph_centrality(G)
    }

    # 2. Create DataFrame to compare rankings - CORRECCIÃ“N AQUÃ
    # Primero creamos un DataFrame con los nodos como columna
    df_ranks = pd.DataFrame({
        'Node': list(G.nodes())
    })

    # Luego agregamos las mÃ©tricas de centralidad
    for name, values in dict_centralities.items():
        # Crear Series con los valores para cada nodo
        value_series = pd.Series(values)

        # AÃ±adir valores
        df_ranks[f'{name}_Val'] = df_ranks['Node'].map(value_series)

        # Crear ranking (1 = most central)
        df_ranks[f'{name}_Rank'] = df_ranks[f'{name}_Val'].rank(ascending=False, method='min')

    # 3. Get Top N for each metric
    top_nodes_dict = {}
    for name in dict_centralities.keys():
        top_list = df_ranks.sort_values(by=f'{name}_Val', ascending=False).head(top_n)['Node'].tolist()
        top_nodes_dict[name] = top_list

    df_top_n = pd.DataFrame(top_nodes_dict)

    # 4. Export results
    df_top_n.to_csv(f"top_{top_n}_centralities.csv", index=False)
    print(f"Top {top_n} saved to 'top_{top_n}_centralities.csv'")

    # TambiÃ©n exportar el DataFrame completo con todos los datos
    df_ranks.to_csv("all_centrality_data.csv", index=False)
    print("All centrality data saved to 'all_centrality_data.csv'")

    return df_ranks, df_top_n

# Execution
# df_full, df_top50 = analyze_centrality_rankings(G)

df_full_G_S03, df_top50_G_S03 = analyze_centrality_rankings(G_S03)

# Guardar DataFrame como CSV
df_full_G_S03.to_csv('G_S03_full_centrality.csv', index=False)

# Open CSV as DataFrame
df_full_G_S03 = pd.read_csv('G_S03_full_centrality.csv')

df_full_G_S03

# Guardar DataFrame como CSV
df_top50_G_S03.to_csv('G_S03_top50_centrality.csv', index=False)

# Open CSV as DataFrame
df_top50_G_S03 = pd.read_csv('G_S03_top50_centrality.csv')

df_top50_G_S03

df_persistencedf_top50_G_S03 = analyze_top_residues_consensus(df_top50_G_S03)

df_chemical_count_G_S03 = plot_physicochemical_centrality_comparison(df_top50_G_S03)

df_key_residues_G_S03 = generate_complete_persistence_analysis(df_top50_G_S03)

#visualize_persistence_3d(df_key_residues_G_S01, pdb_path='./6EE1-R.pdb')
visualize_persistence_3d_clean(df_key_residues_G_S03, pdb_path='./6EE1-R.pdb')

visualize_3d_persistence_pro(df_key_residues_G_S03, pdb_path='./6EE1-R.pdb', show_labels=False)

nearby_residues_MG802 = find_nearby_amino_acids('./6EE1-R.pdb', target_residue="MG", residue_number=802, distance=4.0)
df_nearby_MG802 = pd.DataFrame(nearby_residues_MG802)
df_nearby_MG802

"""#### d) Null models

Null models: generate ten ErdÅ‘sâ€“RÃ©nyi (ER) and ten BarabÃ¡siâ€“Albert (BA) networks with the same number of nodes and edges as your real network.
"""

df_nulls_G_S03 = null_models_analysis(G_S03)
df_nulls_G_S03

plot_null_model_comparison(G_S03)

"""#### f) Community analysis

**Infomap Community analysis**
"""

result_S03 = analyze_and_visualize_infomap(G_S03)

"""**Comparative Community analysis**"""

df_resultados_Community_analysis_G_S03 = comparative_community_analysis(G_S03, "G_S03_mi_analisis_completo.csv")

# Reading the generated Community Data
nombre_archivo_G_S03 = "G_S03_mi_analisis_completo.csv"
df_community_analisis_G_S03 = pd.read_csv(nombre_archivo_G_S03)
df_community_analisis_G_S03

# Definir los mÃ©todos que estÃ¡s analizando
metodos = ['Infomap', 'Modularity', 'Label_Propagation', 'Girvan_Newman', 'K_Clique_3']

# Count community sizes by method and graph them
plt.figure(figsize=(10, 5))
for metodo in metodos:
    counts_G_S03 = df_community_analisis_G_S03[metodo].value_counts().values
    sns.kdeplot(counts_G_S03, label=metodo, fill=True, alpha=0.3)

plt.xlabel("Community Size (Number of Nodes)")
plt.ylabel("Density")
plt.title("Community Size Distribution")
plt.legend()
plt.show()

"""**Consensus analysis between community analysis algorithms**"""

from sklearn.metrics import adjusted_rand_score
import seaborn as sns

# Calculate similarity matrix between columns
metodos = df_community_analisis_G_S03.columns[1:] # Excluimos la columna 'Nodo'
ari_matrix_G_S03 = pd.DataFrame(index=metodos, columns=metodos)

for m1 in metodos:
    for m2 in metodos:
        ari_matrix_G_S03.loc[m1, m2] = adjusted_rand_score(df_community_analisis_G_S03[m1], df_community_analisis_G_S03[m2])

plt.figure(figsize=(8, 6))
sns.heatmap(ari_matrix_G_S03.astype(float), annot=True, cmap='Blues')
plt.title("Similarity betwenn Algoritmos (ARI) in apo form ICL2 structure",
          pad=20)
plt.show()

visualize_local_pdb_communities(df_community_analisis_G_S03, 'Infomap', './6EE1-R.pdb', n_communities=4)

visualize_local_pdb_communities(df_community_analisis_G_S03, 'Girvan_Newman', './6EE1-R.pdb', n_communities=4)



"""**Which nodes are most related?**"""

nodos = df_community_analisis_G_S03['Node'].values
n = len(nodos)
co_occurrence = np.zeros((n, n))

# Iterate through each pair of nodes
for i in range(n):
    for j in range(i + 1, n):
        # Count how many columns have the same values â€‹â€‹in row i and j
        matches = (df_community_analisis_G_S03.iloc[i, 1:] == df_community_analisis_G_S03.iloc[j, 1:]).sum()
        co_occurrence[i, j] = co_occurrence[j, i] = matches

# Visualize a sample or the nodes with the highest relationship
plt.figure(figsize=(10, 8))
sns.heatmap(co_occurrence[:30, :30], xticklabels=nodos[:30], yticklabels=nodos[:30], cmap="YlGnBu")
plt.title("Consensus Matrix (First 30 nodes)")
plt.show()

# Consensus Matrix
matriz_total_G_S03 = generate_consensus_matrix(df_community_analisis_G_S03)

# VisualizaciÃ³n (si n_nodos es manejable, ej. < 200)
plt.figure(figsize=(24, 20))
sns.heatmap(matriz_total_G_S03, cmap="YlGnBu", xticklabels=False, yticklabels=False)
plt.title("Complete Consensus Matrix (All nodes)")
plt.savefig("G_S03_Consensus Matrix.png", dpi=300, bbox_inches='tight')
plt.savefig("G_S03_Consensus Matrix.pdf", bbox_inches='tight')
plt.savefig("G_S03_Consensus Matrix.svg", bbox_inches='tight')
plt.show()

"""**Generate Consensus graph**"""

df_gold_relationships_G_S03, network_gold_relationships_G_S03 = extract_consensus_and_plot(df_community_analisis_G_S03, threshold=0.7)

"""**Umbral analysis in concensus graph**"""

df_gold_relationships_G_S03

df_multi_threshold_G_S03 = generate_multi_threshold_consensus_csv(df_community_analisis_G_S03, thresholds=[1.0, 0.9, 0.8, 0.7, 0.6])

df_multi_threshold_G_S03

plot_unit_count(df_multi_threshold_G_S03)

#Execute Sankey passing that specific dataframe
generate_filtered_sankey(df_multi_threshold_G_S03, min_nodes=5)

# (Opcional) Establecer el nodo como el Ã­ndice para realizar bÃºsquedas rÃ¡pidas
# df_analisis.set_index('Nodo', inplace=True)

# 3. Mostrar las primeras filas y un resumen rÃ¡pido
print("Vista previa del anÃ¡lisis comparativo:")
print(df_community_analisis_G_S03.head())

print("\nConteo de comunidades por cada mÃ©todo:")
print(df_community_analisis_G_S03.nunique())

""" #### e) Structural analysis"""

# Analizar tipos de interacciÃ³n
distribucion_interacciones_S03 = analizar_tipos_interaccion(df_edges_S03, G_S03)

df_bio_G03 = calculate_complete_properties('./6EE1-R.pdb')

df_bio_G03

df_Structural_properties_G_S03 = analyze_allosteric_properties('./6EE1-R.pdb')

df_Structural_properties_G_S03

"""**Betweeness versus B-factor**"""

df_Betweeness_B_factor_G_S03 = plot_network_structure_correlation(df_full_G_S03, df_Structural_properties_G_S03)

"""**Discussion:** That **Spearman ($Ï=âˆ’0.341$)** value with such an extremely low p-value ($3Ã—10^{âˆ’80}$) is irrefutable statistical proof of an organizing principle in the protein: **communication is shielded from flexibility.**

Here is the scientific interpretation:

- **The Meaning of the Negative Correlation**

A negative correlation of -0.341 indicates that information flow (Betweenness) **actively avoids disordered regions**. The protein does not trust "loose wires" to send signals. Instead, it uses a **"high-fidelity backbone."**

* Being rigid (low B-Factor), these residues do not waste energy on random thermal vibrations, allowing a conformational change at one site to be transmitted almost entirely to a distant point.

- **Analysis of the Candidates (The Information "Relays")**

It is very revealing that the top two candidates are **Tryptophans (TRP)**:

* **C/108/TRP and D/457/TRP:** Tryptophan is the most bulky and rigid amino acid. Due to its large aromatic ring, it often acts as a structural "anchor." Their having the highest Betweenness suggests they function as **pivot nodes**. Their rigidity ensures that when the protein moves, these residues act as solid levers that shift entire domains.
* **A/559/GLU and C/527/GLN:** Glutamate and glutamine are polar residues. Their appearance here suggests that rigid communication is not only hydrophobic (in the core), but also involves very stable **hydrogen bond networks** that act as "welds" between different parts of the protein.

**Hypothesis:** If these residues are located between the active site and a ligand-binding site, they form the "railway track" connecting both centers.
"""

site_percolation(G_S03, method='random')